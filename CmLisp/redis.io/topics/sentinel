<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <title>Redis Sentinel Documentation â€“ Redis</title>
    <link href='/styles.css' rel='stylesheet'>
    <link href='/images/favicon.png' rel='shortcut icon'>
    <link href='/opensearch.xml' rel='search' title='Look up a Redis command' type='application/opensearchdescription+xml'>
    <meta content='width=device-width, minimum-scale=1.0, maximum-scale=1.0' name='viewport'>
    <script>
       var _gaq = _gaq || [];
       _gaq.push(['_setAccount', 'UA-20243082-1']);
       _gaq.push(['_trackPageview']);
      
       (function() {
         var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
         ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
         var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
       })();
    </script>
  </head>
  <body class='topics sentinel'>
    <div class='mobile-menu slideout-menu'>
      <header class='menu-header'></header>
      <section class='menu-section'>
        <ul class='menu-section-list'>
          <li>
            <a class='home' href='/'>Home</a>
          </li>
          <li>
            <a href='/commands'>Commands</a>
          </li>
          <li>
            <a href='/clients'>Clients</a>
          </li>
          <li>
            <a href='/documentation'>Documentation</a>
          </li>
          <li>
            <a href='/community'>Community</a>
          </li>
          <li>
            <a href='/download'>Download</a>
          </li>
          <li>
            <a href='/modules'>Modules</a>
          </li>
          <li>
            <a href='/support'>Support</a>
          </li>
        </ul>
      </section>
    </div>
    <div class='site-wrapper'>
      <header class='site-header'>
        <nav class='container'>
          <div class='mobile-header'>
            <button class='btn-hamburger js-slideout-toggle'>
              <span class='fa fa-bars'></span>
            </button>
            <a class='home' href='/'>
              <img alt='Redis' src='/images/redis-white.png'>
            </a>
          </div>
          <div class='desktop-header'>
            <a class='home' href='/'>
              <img alt='Redis' src='/images/redis-white.png'>
            </a>
            <a href='/commands'>Commands</a>
            <a href='/clients'>Clients</a>
            <a href='/documentation'>Documentation</a>
            <a href='/community'>Community</a>
            <a href='/download'>Download</a>
            <a href='/modules'>Modules</a>
            <a href='/support'>Support</a>
          </div>
        </nav>
      </header>
      <div class='site-content'>
        <div class='text'>
          <article id='topic'>
            <span id="redis-sentinel-documentation" class=anchor></span><h1 ><a href="#redis-sentinel-documentation" class=anchor-link>*</a>Redis Sentinel Documentation</h1>
            
            <p>Redis Sentinel provides high availability for Redis. In practical terms this
            means that using Sentinel you can create a Redis deployment that resists
            without human intervention to certain kind of failures.</p>
            
            <p>Redis Sentinel also provides other collateral tasks such as monitoring,
            notifications and acts as a configuration provider for clients.</p>
            
            <p>This is the full list of Sentinel capabilities at a macroscopical level (i.e. the <em>big picture</em>):</p>
            
            <ul>
            <li><strong>Monitoring</strong>. Sentinel constantly checks if your master and slave instances are working as expected.</li>
            <li><strong>Notification</strong>. Sentinel can notify the system administrator, another computer programs, via an API, that something is wrong with one of the monitored Redis instances.</li>
            <li><strong>Automatic failover</strong>. If a master is not working as expected, Sentinel can start a failover process where a slave is promoted to master, the other additional slaves are reconfigured to use the new master, and the applications using the Redis server informed about the new address to use when connecting.</li>
            <li><strong>Configuration provider</strong>. Sentinel acts as a source of authority for clients service discovery: clients connect to Sentinels in order to ask for the address of the current Redis master responsible for a given service. If a failover occurs, Sentinels will report the new address.</li>
            </ul>
            
            <span id="distributed-nature-of-sentinel" class=anchor></span><h2 ><a href="#distributed-nature-of-sentinel" class=anchor-link>*</a>Distributed nature of Sentinel</h2>
            
            <p>Redis Sentinel is a distributed system:</p>
            
            <p>Sentinel itself is designed to run in a configuration where there are multiple Sentinel processes cooperating together. The advantage of having multiple Sentinel processes cooperating are the following:</p>
            
            <ol>
            <li>Failure detection is performed when multiple Sentinels agree about the fact a given master is no longer available. This lowers the probability of false positives.</li>
            <li>Sentinel works even if not all the Sentinel processes are working, making the system robust against failures. There is no fun in having a fail over system which is itself a single point of failure, after all.</li>
            </ol>
            
            <p>The sum of Sentinels, Redis instances (masters and slaves) and clients
            connecting to Sentinel and Redis, are also a larger distributed system with
            specific properties. In this document concepts will be introduced gradually
            starting from basic information needed in order to understand the basic
            properties of Sentinel, to more complex information (that are optional) in
            order to understand how exactly Sentinel works.</p>
            
            <span id="quick-start" class=anchor></span><h1 ><a href="#quick-start" class=anchor-link>*</a>Quick Start</h1>
            
            <span id="obtaining-sentinel" class=anchor></span><h2 ><a href="#obtaining-sentinel" class=anchor-link>*</a>Obtaining Sentinel</h2>
            
            <p>The current version of Sentinel is called <strong>Sentinel 2</strong>. It is a rewrite of
            the initial Sentinel implementation using stronger and simpler to predict
            algorithms (that are explained in this documentation).</p>
            
            <p>A stable release of Redis Sentinel is shipped since Redis 2.8.</p>
            
            <p>New developments are performed in the <em>unstable</em> branch, and new features
            sometimes are back ported into the latest stable branch as soon as they are
            considered to be stable.</p>
            
            <p>Redis Sentinel version 1, shipped with Redis 2.6, is deprecated and should not be used.</p>
            
            <span id="running-sentinel" class=anchor></span><h2 ><a href="#running-sentinel" class=anchor-link>*</a>Running Sentinel</h2>
            
            <p>If you are using the <code>redis-sentinel</code> executable (or if you have a symbolic
            link with that name to the <code>redis-server</code> executable) you can run Sentinel
            with the following command line:</p>
            
            <pre><code>redis-sentinel /path/to/sentinel.conf&#x000A;</code></pre>
            
            <p>Otherwise you can use directly the <code>redis-server</code> executable starting it in
            Sentinel mode:</p>
            
            <pre><code>redis-server /path/to/sentinel.conf --sentinel&#x000A;</code></pre>
            
            <p>Both ways work the same.</p>
            
            <p>However <strong>it is mandatory</strong> to use a configuration file when running Sentinel, as this file will be used by the system in order to save the current state that will be reloaded in case of restarts. Sentinel will simply refuse to start if no configuration file is given or if the configuration file path is not writable.</p>
            
            <p>Sentinels by default run <strong>listening for connections to TCP port 26379</strong>, so
            for Sentinels to work, port 26379 of your servers <strong>must be open</strong> to receive
            connections from the IP addresses of the other Sentinel instances.
            Otherwise Sentinels can&#39;t talk and can&#39;t agree about what to do, so failover
            will never be performed.</p>
            
            <span id="fundamental-things-to-know-about-sentinel-before-deploying" class=anchor></span><h2 ><a href="#fundamental-things-to-know-about-sentinel-before-deploying" class=anchor-link>*</a>Fundamental things to know about Sentinel before deploying</h2>
            
            <ol>
            <li>You need at least three Sentinel instances for a robust deployment.</li>
            <li>The three Sentinel instances should be placed into computers or virtual machines that are believed to fail in an independent way. So for example different physical servers or Virtual Machines executed on different availability zones.</li>
            <li>Sentinel + Redis distributed system does not guarantee that acknowledged writes are retained during failures, since Redis uses asynchronous replication. However there are ways to deploy Sentinel that make the window to lose writes limited to certain moments, while there are other less secure ways to deploy it.</li>
            <li>You need Sentinel support in your clients. Popular client libraries have Sentinel support, but not all.</li>
            <li>There is no HA setup which is safe if you don&#39;t test from time to time in development environments, or even better if you can, in production environments, if they work. You may have a misconfiguration that will become apparent only when it&#39;s too late (at 3am when your master stops working).</li>
            <li><strong>Sentinel, Docker, or other forms of Network Address Translation or Port Mapping should be mixed with care</strong>: Docker performs port remapping, breaking Sentinel auto discovery of other Sentinel processes and the list of slaves for a master. Check the section about Sentinel and Docker later in this document for more information.</li>
            </ol>
            
            <span id="configuring-sentinel" class=anchor></span><h2 ><a href="#configuring-sentinel" class=anchor-link>*</a>Configuring Sentinel</h2>
            
            <p>The Redis source distribution contains a file called <code>sentinel.conf</code>
            that is a self-documented example configuration file you can use to
            configure Sentinel, however a typical minimal configuration file looks like the
            following:</p>
            
            <pre><code>sentinel monitor mymaster 127.0.0.1 6379 2&#x000A;sentinel down-after-milliseconds mymaster 60000&#x000A;sentinel failover-timeout mymaster 180000&#x000A;sentinel parallel-syncs mymaster 1&#x000A;&#x000A;sentinel monitor resque 192.168.1.3 6380 4&#x000A;sentinel down-after-milliseconds resque 10000&#x000A;sentinel failover-timeout resque 180000&#x000A;sentinel parallel-syncs resque 5&#x000A;</code></pre>
            
            <p>You only need to specify the masters to monitor, giving to each separated
            master (that may have any number of slaves) a different name. There is no
            need to specify slaves, which are auto-discovered. Sentinel will update the
            configuration automatically with additional information about slaves (in
            order to retain the information in case of restart). The configuration is
            also rewritten every time a slave is promoted to master during a failover
            and every time a new Sentinel is discovered.</p>
            
            <p>The example configuration above, basically monitor two sets of Redis
            instances, each composed of a master and an undefined number of slaves.
            One set of instances is called <code>mymaster</code>, and the other <code>resque</code>.</p>
            
            <p>The meaning of the arguments of <code>sentinel monitor</code> statements is the following:</p>
            
            <pre><code>sentinel monitor &lt;master-group-name&gt; &lt;ip&gt; &lt;port&gt; &lt;quorum&gt;&#x000A;</code></pre>
            
            <p>For the sake of clarity, let&#39;s check line by line what the configuration
            options mean:</p>
            
            <p>The first line is used to tell Redis to monitor a master called <em>mymaster</em>,
            that is at address 127.0.0.1 and port 6379, with a quorum of 2. Everything
            is pretty obvious but the <strong>quorum</strong> argument:</p>
            
            <ul>
            <li>The <strong>quorum</strong> is the number of Sentinels that need to agree about the fact the master is not reachable, in order for really mark the slave as failing, and eventually start a fail over procedure if possible.</li>
            <li>However <strong>the quorum is only used to detect the failure</strong>. In order to actually perform a failover, one of the Sentinels need to be elected leader for the failover and be authorized to proceed. This only happens with the vote of the <strong>majority of the Sentinel processes</strong>.</li>
            </ul>
            
            <p>So for example if you have 5 Sentinel processes, and the quorum for a given
            master set to the value of 2, this is what happens:</p>
            
            <ul>
            <li>If two Sentinels agree at the same time about the master being unreachable, one of the two will try to start a failover.</li>
            <li>If there are at least a total of three Sentinels reachable, the failover will be authorized and will actually start.</li>
            </ul>
            
            <p>In practical terms this means during failures <strong>Sentinel never starts a failover if the majority of Sentinel processes are unable to talk</strong> (aka no failover in the minority partition).</p>
            
            <span id="other-sentinel-options" class=anchor></span><h2 ><a href="#other-sentinel-options" class=anchor-link>*</a>Other Sentinel options</h2>
            
            <p>The other options are almost always in the form:</p>
            
            <pre><code>sentinel &lt;option_name&gt; &lt;master_name&gt; &lt;option_value&gt;&#x000A;</code></pre>
            
            <p>And are used for the following purposes:</p>
            
            <ul>
            <li><code>down-after-milliseconds</code> is the time in milliseconds an instance should not
            be reachable (either does not reply to our PINGs or it is replying with an
            error) for a Sentinel starting to think it is down.</li>
            <li><code>parallel-syncs</code> sets the number of slaves that can be reconfigured to use
            the new master after a failover at the same time. The lower the number, the
            more time it will take for the failover process to complete, however if the
            slaves are configured to serve old data, you may not want all the slaves to
            re-synchronize with the master at the same time. While the replication
            process is mostly non blocking for a slave, there is a moment when it stops to
            load the bulk data from the master. You may want to make sure only one slave
            at a time is not reachable by setting this option to the value of 1.</li>
            </ul>
            
            <p>Additional options are described in the rest of this document and
            documented in the example <code>sentinel.conf</code> file shipped with the Redis
            distribution.</p>
            
            <p>All the configuration parameters can be modified at runtime using the <code>SENTINEL SET</code> command. See the <strong>Reconfiguring Sentinel at runtime</strong> section for more information.</p>
            
            <span id="example-sentinel-deployments" class=anchor></span><h2 ><a href="#example-sentinel-deployments" class=anchor-link>*</a>Example Sentinel deployments</h2>
            
            <p>Now that you know the basic information about Sentinel, you may wonder where
            you should place your Sentinel processes, how much Sentinel processes you need
            and so forth. This section shows a few example deployments.</p>
            
            <p>We use ASCII art in order to show you configuration examples in a <em>graphical</em>
            format, this is what the different symbols means:</p>
            
            <pre><code>+--------------------+&#x000A;| This is a computer |&#x000A;| or VM that fails   |&#x000A;| independently. We  |&#x000A;| call it a &quot;box&quot;    |&#x000A;+--------------------+&#x000A;</code></pre>
            
            <p>We write inside the boxes what they are running:</p>
            
            <pre><code>+-------------------+&#x000A;| Redis master M1   |&#x000A;| Redis Sentinel S1 |&#x000A;+-------------------+&#x000A;</code></pre>
            
            <p>Different boxes are connected by lines, to show that they are able to talk:</p>
            
            <pre><code>+-------------+               +-------------+&#x000A;| Sentinel S1 |---------------| Sentinel S2 |&#x000A;+-------------+               +-------------+&#x000A;</code></pre>
            
            <p>Network partitions are shown as interrupted lines using slashes:</p>
            
            <pre><code>+-------------+                +-------------+&#x000A;| Sentinel S1 |------ // ------| Sentinel S2 |&#x000A;+-------------+                +-------------+&#x000A;</code></pre>
            
            <p>Also note that:</p>
            
            <ul>
            <li>Masters are called M1, M2, M3, ..., Mn.</li>
            <li>Slaves are called R1, R2, R3, ..., Rn (R stands for <em>replica</em>).</li>
            <li>Sentinels are called S1, S2, S3, ..., Sn.</li>
            <li>Clients are called C1, C2, C3, ..., Cn.</li>
            <li>When an instance changes role because of Sentinel actions, we put it inside square brackets, so [M1] means an instance that is now a master because of Sentinel intervention.</li>
            </ul>
            
            <p>Note that we will never show <strong>setups where just two Sentinels are used</strong>, since
            Sentinels always need <strong>to talk with the majority</strong> in order to start a
            failover.</p>
            
            <span id="example-1-just-two-sentinels-don39t-do-this" class=anchor></span><h2 ><a href="#example-1-just-two-sentinels-don39t-do-this" class=anchor-link>*</a>Example 1: just two Sentinels, DON&#39;T DO THIS</h2>
            
            <pre><code>+----+         +----+&#x000A;| M1 |---------| R1 |&#x000A;| S1 |         | S2 |&#x000A;+----+         +----+&#x000A;&#x000A;Configuration: quorum = 1&#x000A;</code></pre>
            
            <ul>
            <li>In this setup, if the master M1 fails, R1 will be promoted since the two Sentinels can reach agreement about the failure (obviously with quorum set to 1) and can also authorize a failover because the majority is two. So apparently it could superficially work, however check the next points to see why this setup is broken.</li>
            <li>If the box where M1 is running stops working, also S1 stops working. The Sentinel running in the other box S2 will not be able to authorize a failover, so the system will become not available.</li>
            </ul>
            
            <p>Note that a majority is needed in order to order different failovers, and later propagate the latest configuration to all the Sentinels. Also note that the ability to failover in a single side of the above setup, without any agreement, would be very dangerous:</p>
            
            <pre><code>+----+           +------+&#x000A;| M1 |----//-----| [M1] |&#x000A;| S1 |           | S2   |&#x000A;+----+           +------+&#x000A;</code></pre>
            
            <p>In the above configuration we created two masters (assuming S2 could failover
            without authorization) in a perfectly symmetrical way. Clients may write
            indefinitely to both sides, and there is no way to understand when the
            partition heals what configuration is the right one, in order to prevent
            a <em>permanent split brain condition</em>.</p>
            
            <p>So please <strong>deploy at least three Sentinels in three different boxes</strong> always.</p>
            
            <span id="example-2-basic-setup-with-three-boxes" class=anchor></span><h2 ><a href="#example-2-basic-setup-with-three-boxes" class=anchor-link>*</a>Example 2: basic setup with three boxes</h2>
            
            <p>This is a very simple setup, that has the advantage to be simple to tune
            for additional safety. It is based on three boxes, each box running both
            a Redis process and a Sentinel process.</p>
            
            <pre><code>       +----+&#x000A;       | M1 |&#x000A;       | S1 |&#x000A;       +----+&#x000A;          |&#x000A;+----+    |    +----+&#x000A;| R2 |----+----| R3 |&#x000A;| S2 |         | S3 |&#x000A;+----+         +----+&#x000A;&#x000A;Configuration: quorum = 2&#x000A;</code></pre>
            
            <p>If the master M1 fails, S2 and S3 will agree about the failure and will
            be able to authorize a failover, making clients able to continue.</p>
            
            <p>In every Sentinel setup, being Redis asynchronously replicated, there is
            always the risk of losing some write because a given acknowledged write
            may not be able to reach the slave which is promoted to master. However in
            the above setup there is an higher risk due to clients partitioned away
            with an old master, like in the following picture:</p>
            
            <pre><code>         +----+&#x000A;         | M1 |&#x000A;         | S1 | &lt;- C1 (writes will be lost)&#x000A;         +----+&#x000A;            |&#x000A;            /&#x000A;            /&#x000A;+------+    |    +----+&#x000A;| [M2] |----+----| R3 |&#x000A;| S2   |         | S3 |&#x000A;+------+         +----+&#x000A;</code></pre>
            
            <p>In this case a network partition isolated the old master M1, so the
            slave R2 is promoted to master. However clients, like C1, that are
            in the same partition as the old master, may continue to write data
            to the old master. This data will be lost forever since when the partition
            will heal, the master will be reconfigured as a slave of the new master,
            discarding its data set.</p>
            
            <p>This problem can be mitigated using the following Redis replication
            feature, that allows to stop accepting writes if a master detects that
            is no longer able to transfer its writes to the specified number of slaves.</p>
            
            <pre><code>min-slaves-to-write 1&#x000A;min-slaves-max-lag 10&#x000A;</code></pre>
            
            <p>With the above configuration (please see the self-commented <code>redis.conf</code> example in the Redis distribution for more information) a Redis instance, when acting as a master, will stop accepting writes if it can&#39;t write to at least 1 slave. Since replication is asynchronous <em>not being able to write</em> actually means that the slave is either disconnected, or is not sending us asynchronous acknowledges for more than the specified <code>max-lag</code> number of seconds.</p>
            
            <p>Using this configuration the old Redis master M1 in the above example, will become unavailable after 10 seconds. When the partition heals, the Sentinel configuration will converge to the new one, the client C1 will be able to fetch a valid configuration and will continue with the new master.</p>
            
            <p>However there is no free lunch. With this refinement, if the two slaves are
            down, the master will stop accepting writes. It&#39;s a trade off.</p>
            
            <span id="example-3-sentinel-in-the-client-boxes" class=anchor></span><h2 ><a href="#example-3-sentinel-in-the-client-boxes" class=anchor-link>*</a>Example 3: Sentinel in the client boxes</h2>
            
            <p>Sometimes we have only two Redis boxes available, one for the master and
            one for the slave. The configuration in the example 2 is not viable in
            that case, so we can resort to the following, where Sentinels are placed
            where clients are:</p>
            
            <pre><code>            +----+         +----+&#x000A;            | M1 |----+----| R1 |&#x000A;            |    |    |    |    |&#x000A;            +----+    |    +----+&#x000A;                      |&#x000A;         +------------+------------+&#x000A;         |            |            |&#x000A;         |            |            |&#x000A;      +----+        +----+      +----+&#x000A;      | C1 |        | C2 |      | C3 |&#x000A;      | S1 |        | S2 |      | S3 |&#x000A;      +----+        +----+      +----+&#x000A;&#x000A;      Configuration: quorum = 2&#x000A;</code></pre>
            
            <p>In this setup, the point of view Sentinels is the same as the clients: if
            a master is reachable by the majority of the clients, it is fine.
            C1, C2, C3 here are generic clients, it does not mean that C1 identifies
            a single client connected to Redis. It is more likely something like
            an application server, a Rails app, or something like that.</p>
            
            <p>If the box where M1 and S1 are running fails, the failover will happen
            without issues, however it is easy to see that different network partitions
            will result in different behaviors. For example Sentinel will not be able
            to setup if the network between the clients and the Redis servers will
            get disconnected, since the Redis master and slave will be both not
            available.</p>
            
            <p>Note that if C3 gets partitioned with M1 (hardly possible with
            the network described above, but more likely possible with different
            layouts, or because of failures at the software layer), we have a similar
            issue as described in Example 2, with the difference that here we have
            no way to break the symmetry, since there is just a slave and master, so
            the master can&#39;t stop accepting queries when it is disconnected from its slave,
            otherwise the master would never be available during slave failures.</p>
            
            <p>So this is a valid setup but the setup in the Example 2 has advantages
            such as the HA system of Redis running in the same boxes as Redis itself
            which may be simpler to manage, and the ability to put a bound on the amount
            of time a master into the minority partition can receive writes.</p>
            
            <span id="example-4-sentinel-client-side-with-less-than-three-clients" class=anchor></span><h2 ><a href="#example-4-sentinel-client-side-with-less-than-three-clients" class=anchor-link>*</a>Example 4: Sentinel client side with less than three clients</h2>
            
            <p>The setup described in the Example 3 cannot be used if there are not enough
            three boxes in the client side (for example three web servers). In this
            case we need to resort to a mixed setup like the following:</p>
            
            <pre><code>            +----+         +----+&#x000A;            | M1 |----+----| R1 |&#x000A;            | S1 |    |    | S2 |&#x000A;            +----+    |    +----+&#x000A;                      |&#x000A;               +------+-----+&#x000A;               |            |  &#x000A;               |            |&#x000A;            +----+        +----+&#x000A;            | C1 |        | C2 |&#x000A;            | S3 |        | S4 |&#x000A;            +----+        +----+&#x000A;&#x000A;      Configuration: quorum = 3&#x000A;</code></pre>
            
            <p>This is similar to the setup in Example 3, but here we run four Sentinels
            in the four boxes we have available. If the master M1 becomes not available
            the other three Sentinels will perform the failover.</p>
            
            <p>In theory this setup works removing the box where C2 and S4 are running, and
            setting the quorum to 2. However it is unlikely that we want HA in the
            Redis side without having high availability in our application layer.</p>
            
            <span id="sentinel-docker-nat-and-possible-issues" class=anchor></span><h2 ><a href="#sentinel-docker-nat-and-possible-issues" class=anchor-link>*</a>Sentinel, Docker, NAT, and possible issues</h2>
            
            <p>Docker uses a technique called port mapping: programs running inside Docker
            containers may be exposed with a different port compared to the one the
            program believes to be using. This is useful in order to run multiple
            containers using the same ports, at the same time, in the same server.</p>
            
            <p>Docker is not the only software system where this happens, there are other
            Network Address Translation setups where ports may be remapped, and sometimes
            not ports but also IP addresses.</p>
            
            <p>Remapping ports and addresses creates issues with Sentinel in two ways:</p>
            
            <ol>
            <li>Sentinel auto-discovery of other Sentinels no longer works, since it is based on <em>hello</em> messages where each Sentinel announce at which port and IP address they are listening for connection. However Sentinels have no way to understand that an address or port is remapped, so it is announcing an information that is not correct for other Sentinels to connect.</li>
            <li>Slaves are listed in the <a href="/commands/info">INFO</a> output of a Redis master in a similar way: the address is detected by the master checking the remote peer of the TCP connection, while the port is advertised by the slave itself during the handshake, however the port may be wrong for the same reason as exposed in point 1.</li>
            </ol>
            
            <p>Since Sentinels auto detect slaves using masters <a href="/commands/info">INFO</a> output information,
            the detected slaves will not be reachable, and Sentinel will never be able to
            failover the master, since there are no good slaves from the point of view of
            the system, so there is currently no way to monitor with Sentinel a set of
            master and slave instances deployed with Docker, <strong>unless you instruct Docker
            to map the port 1:1</strong>.</p>
            
            <p>For the first problem, in case you want to run a set of Sentinel
            instances using Docker with forwarded ports (or any other NAT setup where ports
            are remapped), you can use the following two Sentinel configuration directives
            in order to force Sentinel to announce a specific set of IP and port:</p>
            
            <pre><code>sentinel announce-ip &lt;ip&gt;&#x000A;sentinel announce-port &lt;port&gt;&#x000A;</code></pre>
            
            <p>Note that Docker has the ability to run in <em>host networking mode</em> (check the <code>--net=host</code> option for more information). This should create no issues since ports are not remapped in this setup.</p>
            
            <span id="a-quick-tutorial" class=anchor></span><h1 ><a href="#a-quick-tutorial" class=anchor-link>*</a>A quick tutorial</h1>
            
            <p>In the next sections of this document, all the details about Sentinel API,
            configuration and semantics will be covered incrementally. However for people
            that want to play with the system ASAP, this section is a tutorial that shows
            how to configure and interact with 3 Sentinel instances.</p>
            
            <p>Here we assume that the instances are executed at port 5000, 5001, 5002.
            We also assume that you have a running Redis master at port 6379 with a
            slave running at port 6380. We will use the IPv4 loopback address 127.0.0.1
            everywhere during the tutorial, assuming you are running the simulation
            on your personal computer.</p>
            
            <p>The three Sentinel configuration files should look like the following:</p>
            
            <pre><code>port 5000&#x000A;sentinel monitor mymaster 127.0.0.1 6379 2&#x000A;sentinel down-after-milliseconds mymaster 5000&#x000A;sentinel failover-timeout mymaster 60000&#x000A;sentinel parallel-syncs mymaster 1&#x000A;</code></pre>
            
            <p>The other two configuration files will be identical but using 5001 and 5002
            as port numbers.</p>
            
            <p>A few things to note about the above configuration:</p>
            
            <ul>
            <li>The master set is called <code>mymaster</code>. It identifies the master and its slaves. Since each <em>master set</em> has a different name, Sentinel can monitor different sets of masters and slaves at the same time.</li>
            <li>The quorum was set to the value of 2 (last argument of <code>sentinel monitor</code> configuration directive).</li>
            <li>The <code>down-after-milliseconds</code> value is 5000 milliseconds, that is 5 seconds, so masters will be detected as failing as soon as we don&#39;t receive any reply from our pings within this amount of time.</li>
            </ul>
            
            <p>Once you start the three Sentinels, you&#39;ll see a few messages they log, like:</p>
            
            <pre><code>+monitor master mymaster 127.0.0.1 6379 quorum 2&#x000A;</code></pre>
            
            <p>This is a Sentinel event, and you can receive this kind of events via Pub/Sub
            if you <a href="/commands/subscribe">SUBSCRIBE</a> to the event name as specified later.</p>
            
            <p>Sentinel generates and logs different events during failure detection and
            failover.</p>
            
            <span id="asking-sentinel-about-the-state-of-a-master" class=anchor></span><h2 ><a href="#asking-sentinel-about-the-state-of-a-master" class=anchor-link>*</a>Asking Sentinel about the state of a master</h2>
            
            <p>The most obvious thing to do with Sentinel to get started, is check if the
            master it is monitoring is doing well:</p>
            
            <pre><code>$ redis-cli -p 5000&#x000A;127.0.0.1:5000&gt; sentinel master mymaster&#x000A; 1) &quot;name&quot;&#x000A; 2) &quot;mymaster&quot;&#x000A; 3) &quot;ip&quot;&#x000A; 4) &quot;127.0.0.1&quot;&#x000A; 5) &quot;port&quot;&#x000A; 6) &quot;6379&quot;&#x000A; 7) &quot;runid&quot;&#x000A; 8) &quot;953ae6a589449c13ddefaee3538d356d287f509b&quot;&#x000A; 9) &quot;flags&quot;&#x000A;10) &quot;master&quot;&#x000A;11) &quot;link-pending-commands&quot;&#x000A;12) &quot;0&quot;&#x000A;13) &quot;link-refcount&quot;&#x000A;14) &quot;1&quot;&#x000A;15) &quot;last-ping-sent&quot;&#x000A;16) &quot;0&quot;&#x000A;17) &quot;last-ok-ping-reply&quot;&#x000A;18) &quot;735&quot;&#x000A;19) &quot;last-ping-reply&quot;&#x000A;20) &quot;735&quot;&#x000A;21) &quot;down-after-milliseconds&quot;&#x000A;22) &quot;5000&quot;&#x000A;23) &quot;info-refresh&quot;&#x000A;24) &quot;126&quot;&#x000A;25) &quot;role-reported&quot;&#x000A;26) &quot;master&quot;&#x000A;27) &quot;role-reported-time&quot;&#x000A;28) &quot;532439&quot;&#x000A;29) &quot;config-epoch&quot;&#x000A;30) &quot;1&quot;&#x000A;31) &quot;num-slaves&quot;&#x000A;32) &quot;1&quot;&#x000A;33) &quot;num-other-sentinels&quot;&#x000A;34) &quot;2&quot;&#x000A;35) &quot;quorum&quot;&#x000A;36) &quot;2&quot;&#x000A;37) &quot;failover-timeout&quot;&#x000A;38) &quot;60000&quot;&#x000A;39) &quot;parallel-syncs&quot;&#x000A;40) &quot;1&quot;&#x000A;</code></pre>
            
            <p>As you can see, it prints a number of information about the master. There are
            a few that are of particular interest for us:</p>
            
            <ol>
            <li><code>num-other-sentinels</code> is 2, so we know the Sentinel already detected two more Sentinels for this master. If you check the logs you&#39;ll see the <code>+sentinel</code> events generated.</li>
            <li><code>flags</code> is just <code>master</code>. If the master was down we could expect to see <code>s_down</code> or <code>o_down</code> flag as well here.</li>
            <li><code>num-slaves</code> is correctly set to 1, so Sentinel also detected that there is an attached slave to our master.</li>
            </ol>
            
            <p>In order to explore more about this instance, you may want to try the following
            two commands:</p>
            
            <pre><code>SENTINEL slaves mymaster&#x000A;SENTINEL sentinels mymaster&#x000A;</code></pre>
            
            <p>The first will provide similar information about the slaves connected to the
            master, and the second about the other Sentinels.</p>
            
            <span id="obtaining-the-address-of-the-current-master" class=anchor></span><h2 ><a href="#obtaining-the-address-of-the-current-master" class=anchor-link>*</a>Obtaining the address of the current master</h2>
            
            <p>As we already specified, Sentinel also acts as a configuration provider for
            clients that want to connect to a set of master and slaves. Because of
            possible failovers or reconfigurations, clients have no idea about who is
            the currently active master for a given set of instances, so Sentinel exports
            an API to ask this question:</p>
            
            <pre><code>127.0.0.1:5000&gt; SENTINEL get-master-addr-by-name mymaster&#x000A;1) &quot;127.0.0.1&quot;&#x000A;2) &quot;6379&quot;&#x000A;</code></pre>
            
            <span id="testing-the-failover" class=anchor></span><h2 ><a href="#testing-the-failover" class=anchor-link>*</a>Testing the failover</h2>
            
            <p>At this point our toy Sentinel deployment is ready to be tested. We can
            just kill our master and check if the configuration changes. To do so
            we can just do:</p>
            
            <pre><code>redis-cli -p 6379 DEBUG sleep 30&#x000A;</code></pre>
            
            <p>This command will make our master no longer reachable, sleeping for 30 seconds.
            It basically simulates a master hanging for some reason.</p>
            
            <p>If you check the Sentinel logs, you should be able to see a lot of action:</p>
            
            <ol>
            <li>Each Sentinel detects the master is down with an <code>+sdown</code> event.</li>
            <li>This event is later escalated to <code>+odown</code>, which means that multiple Sentinels agree about the fact the master is not reachable.</li>
            <li>Sentinels vote a Sentinel that will start the first failover attempt.</li>
            <li>The failover happens.</li>
            </ol>
            
            <p>If you ask again what is the current master address for <code>mymaster</code>, eventually
            we should get a different reply this time:</p>
            
            <pre><code>127.0.0.1:5000&gt; SENTINEL get-master-addr-by-name mymaster&#x000A;1) &quot;127.0.0.1&quot;&#x000A;2) &quot;6380&quot;&#x000A;</code></pre>
            
            <p>So far so good... At this point you may jump to create your Sentinel deployment
            or can read more to understand all the Sentinel commands and internals.</p>
            
            <span id="sentinel-api" class=anchor></span><h1 ><a href="#sentinel-api" class=anchor-link>*</a>Sentinel API</h1>
            
            <p>Sentinel provides an API in order to inspect its state, check the health
            of monitored masters and slaves, subscribe in order to receive specific
            notifications, and change the Sentinel configuration at run time.</p>
            
            <p>By default Sentinel runs using TCP port 26379 (note that 6379 is the normal
            Redis port). Sentinels accept commands using the Redis protocol, so you can
            use <code>redis-cli</code> or any other unmodified Redis client in order to talk with
            Sentinel.</p>
            
            <p>It is possible to directly query a Sentinel to check what is the state of
            the monitored Redis instances from its point of view, to see what other
            Sentinels it knows, and so forth. Alternatively, using Pub/Sub, it is possible
            to receive <em>push style</em> notifications from Sentinels, every time some event
            happens, like a failover, or an instance entering an error condition, and
            so forth.</p>
            
            <span id="sentinel-commands" class=anchor></span><h2 ><a href="#sentinel-commands" class=anchor-link>*</a>Sentinel commands</h2>
            
            <p>The following is a list of accepted commands, not covering commands used in
            order to modify the Sentinel configuration, which are covered later.</p>
            
            <ul>
            <li><strong>PING</strong> This command simply returns PONG.</li>
            <li><strong>SENTINEL masters</strong> Show a list of monitored masters and their state.</li>
            <li><strong>SENTINEL master <code>&lt;master name&gt;</code></strong> Show the state and info of the specified master.</li>
            <li><strong>SENTINEL slaves <code>&lt;master name&gt;</code></strong> Show a list of slaves for this master, and their state.</li>
            <li><strong>SENTINEL sentinels <code>&lt;master name&gt;</code></strong> Show a list of sentinel instances for this master, and their state.</li>
            <li><strong>SENTINEL get-master-addr-by-name <code>&lt;master name&gt;</code></strong> Return the ip and port number of the master with that name. If a failover is in progress or terminated successfully for this master it returns the address and port of the promoted slave.</li>
            <li><strong>SENTINEL reset <code>&lt;pattern&gt;</code></strong> This command will reset all the masters with matching name. The pattern argument is a glob-style pattern. The reset process clears any previous state in a master (including a failover in progress), and removes every slave and sentinel already discovered and associated with the master.</li>
            <li><strong>SENTINEL failover <code>&lt;master name&gt;</code></strong> Force a failover as if the master was not reachable, and without asking for agreement to other Sentinels (however a new version of the configuration will be published so that the other Sentinels will update their configurations).</li>
            <li><strong>SENTINEL ckquorum <code>&lt;master name&gt;</code></strong> Check if the current Sentinel configuration is able to reach the quorum needed to failover a master, and the majority needed to authorize the failover. This command should be used in monitoring systems to check if a Sentinel deployment is ok.</li>
            <li><strong>SENTINEL flushconfig</strong> Force Sentinel to rewrite its configuration on disk, including the current Sentinel state. Normally Sentinel rewrites the configuration every time something changes in its state (in the context of the subset of the state which is persisted on disk across restart). However sometimes it is possible that the configuration file is lost because of operation errors, disk failures, package upgrade scripts or configuration managers. In those cases a way to to force Sentinel to rewrite the configuration file is handy. This command works even if the previous configuration file is completely missing.</li>
            </ul>
            
            <span id="reconfiguring-sentinel-at-runtime" class=anchor></span><h2 ><a href="#reconfiguring-sentinel-at-runtime" class=anchor-link>*</a>Reconfiguring Sentinel at Runtime</h2>
            
            <p>Starting with Redis version 2.8.4, Sentinel provides an API in order to add, remove, or change the configuration of a given master. Note that if you have multiple sentinels you should apply the changes to all to your instances for Redis Sentinel to work properly. This means that changing the configuration of a single Sentinel does not automatically propagates the changes to the other Sentinels in the network.</p>
            
            <p>The following is a list of <code>SENTINEL</code> sub commands used in order to update the configuration of a Sentinel instance.</p>
            
            <ul>
            <li><strong>SENTINEL MONITOR <code>&lt;name&gt;</code> <code>&lt;ip&gt;</code> <code>&lt;port&gt;</code> <code>&lt;quorum&gt;</code></strong> This command tells the Sentinel to start monitoring a new master with the specified name, ip, port, and quorum. It is identical to the <code>sentinel monitor</code> configuration directive in <code>sentinel.conf</code> configuration file, with the difference that you can&#39;t use an hostname in as <code>ip</code>, but you need to provide an IPv4 or IPv6 address.</li>
            <li><strong>SENTINEL REMOVE <code>&lt;name&gt;</code></strong> is used in order to remove the specified master: the master will no longer be monitored, and will totally be removed from the internal state of the Sentinel, so it will no longer listed by <code>SENTINEL masters</code> and so forth.</li>
            <li><strong>SENTINEL SET <code>&lt;name&gt;</code> <code>&lt;option&gt;</code> <code>&lt;value&gt;</code></strong> The SET command is very similar to the <a href="/commands/config-set">CONFIG SET</a> command of Redis, and is used in order to change configuration parameters of a specific master. Multiple option / value pairs can be specified (or none at all). All the configuration parameters that can be configured via <code>sentinel.conf</code> are also configurable using the SET command.</li>
            </ul>
            
            <p>The following is an example of <code>SENTINEL SET</code> command in order to modify the <code>down-after-milliseconds</code> configuration of a master called <code>objects-cache</code>:</p>
            
            <pre><code>SENTINEL SET objects-cache-master down-after-milliseconds 1000&#x000A;</code></pre>
            
            <p>As already stated, <code>SENTINEL SET</code> can be used to set all the configuration parameters that are settable in the startup configuration file. Moreover it is possible to change just the master quorum configuration without removing and re-adding the master with <code>SENTINEL REMOVE</code> followed by <code>SENTINEL MONITOR</code>, but simply using:</p>
            
            <pre><code>SENTINEL SET objects-cache-master quorum 5&#x000A;</code></pre>
            
            <p>Note that there is no equivalent GET command since <code>SENTINEL MASTER</code> provides all the configuration parameters in a simple to parse format (as a field/value pairs array).</p>
            
            <span id="adding-or-removing-sentinels" class=anchor></span><h2 ><a href="#adding-or-removing-sentinels" class=anchor-link>*</a>Adding or removing Sentinels</h2>
            
            <p>Adding a new Sentinel to your deployment is a simple process because of the
            auto-discover mechanism implemented by Sentinel. All you need to do is to
            start the new Sentinel configured to monitor the currently active master.
            Within 10 seconds the Sentinel will acquire the list of other Sentinels and
            the set of slaves attached to the master.</p>
            
            <p>If you need to add multiple Sentinels at once, it is suggested to add it
            one after the other, waiting for all the other Sentinels to already know
            about the first one before adding the next. This is useful in order to still
            guarantee that majority can be achieved only in one side of a partition,
            in the chance failures should happen in the process of adding new Sentinels.</p>
            
            <p>This can be easily achieved by adding every new Sentinel with a 30 seconds delay, and during absence of network partitions.</p>
            
            <p>At the end of the process it is possible to use the command
            <code>SENTINEL MASTER mastername</code> in order to check if all the Sentinels agree about
            the total number of Sentinels monitoring the master.</p>
            
            <p>Removing a Sentinel is a bit more complex: <strong>Sentinels never forget already seen
            Sentinels</strong>, even if they are not reachable for a long time, since we don&#39;t
            want to dynamically change the majority needed to authorize a failover and
            the creation of a new configuration number. So in order to remove a Sentinel
            the following steps should be performed in absence of network partitions:</p>
            
            <ol>
            <li>Stop the Sentinel process of the Sentinel you want to remove.</li>
            <li>Send a <code>SENTINEL RESET *</code> command to all the other Sentinel instances (instead of <code>*</code> you can use the exact master name if you want to reset just a single master). One after the other, waiting at least 30 seconds between instances.</li>
            <li>Check that all the Sentinels agree about the number of Sentinels currently active, by inspecting the output of <code>SENTINEL MASTER mastername</code> of every Sentinel.</li>
            </ol>
            
            <span id="removing-the-old-master-or-unreachable-slaves" class=anchor></span><h2 ><a href="#removing-the-old-master-or-unreachable-slaves" class=anchor-link>*</a>Removing the old master or unreachable slaves</h2>
            
            <p>Sentinels never forget about slaves of a given master, even when they are
            unreachable for a long time. This is useful, because Sentinels should be able
            to correctly reconfigure a returning slave after a network partition or a
            failure event.</p>
            
            <p>Moreover, after a failover, the failed over master is virtually added as a
            slave of the new master, this way it will be reconfigured to replicate with
            the new master as soon as it will be available again.</p>
            
            <p>However sometimes you want to remove a slave (that may be the old master)
            forever from the list of slaves monitored by Sentinels.</p>
            
            <p>In order to do this, you need to send a <code>SENTINEL RESET mastername</code> command
            to all the Sentinels: they&#39;ll refresh the list of slaves within the next
            10 seconds, only adding the ones listed as correctly replicating from the
            current master <a href="/commands/info">INFO</a> output.</p>
            
            <span id="pubsub-messages" class=anchor></span><h2 ><a href="#pubsub-messages" class=anchor-link>*</a>Pub/Sub Messages</h2>
            
            <p>A client can use a Sentinel as it was a Redis compatible Pub/Sub server
            (but you can&#39;t use <a href="/commands/publish">PUBLISH</a>) in order to <a href="/commands/subscribe">SUBSCRIBE</a> or <a href="/commands/psubscribe">PSUBSCRIBE</a> to
            channels and get notified about specific events.</p>
            
            <p>The channel name is the same as the name of the event. For instance the
            channel named <code>+sdown</code> will receive all the notifications related to instances
            entering an <code>SDOWN</code> (SDOWN means the instance is no longer reachable from
            the point of view of the Sentinel you are querying) condition.</p>
            
            <p>To get all the messages simply subscribe using <code>PSUBSCRIBE *</code>.</p>
            
            <p>The following is a list of channels and message formats you can receive using
            this API. The first word is the channel / event name, the rest is the format of the data.</p>
            
            <p>Note: where <em>instance details</em> is specified it means that the following arguments are provided to identify the target instance:</p>
            
            <pre><code>&lt;instance-type&gt; &lt;name&gt; &lt;ip&gt; &lt;port&gt; @ &lt;master-name&gt; &lt;master-ip&gt; &lt;master-port&gt;&#x000A;</code></pre>
            
            <p>The part identifying the master (from the @ argument to the end) is optional
            and is only specified if the instance is not a master itself.</p>
            
            <ul>
            <li><strong>+reset-master</strong> <code>&lt;instance details&gt;</code> -- The master was reset.</li>
            <li><strong>+slave</strong> <code>&lt;instance details&gt;</code> -- A new slave was detected and attached.</li>
            <li><strong>+failover-state-reconf-slaves</strong> <code>&lt;instance details&gt;</code> -- Failover state changed to <code>reconf-slaves</code> state.</li>
            <li><strong>+failover-detected</strong> <code>&lt;instance details&gt;</code> -- A failover started by another Sentinel or any other external entity was detected (An attached slave turned into a master).</li>
            <li><strong>+slave-reconf-sent</strong> <code>&lt;instance details&gt;</code> -- The leader sentinel sent the <a href="/commands/slaveof">SLAVEOF</a> command to this instance in order to reconfigure it for the new slave.</li>
            <li><strong>+slave-reconf-inprog</strong> <code>&lt;instance details&gt;</code> -- The slave being reconfigured showed to be a slave of the new master ip:port pair, but the synchronization process is not yet complete.</li>
            <li><strong>+slave-reconf-done</strong> <code>&lt;instance details&gt;</code> -- The slave is now synchronized with the new master.</li>
            <li><strong>-dup-sentinel</strong> <code>&lt;instance details&gt;</code> -- One or more sentinels for the specified master were removed as duplicated (this happens for instance when a Sentinel instance is restarted).</li>
            <li><strong>+sentinel</strong> <code>&lt;instance details&gt;</code> -- A new sentinel for this master was detected and attached.</li>
            <li><strong>+sdown</strong> <code>&lt;instance details&gt;</code> -- The specified instance is now in Subjectively Down state.</li>
            <li><strong>-sdown</strong> <code>&lt;instance details&gt;</code> -- The specified instance is no longer in Subjectively Down state.</li>
            <li><strong>+odown</strong> <code>&lt;instance details&gt;</code> -- The specified instance is now in Objectively Down state.</li>
            <li><strong>-odown</strong> <code>&lt;instance details&gt;</code> -- The specified instance is no longer in Objectively Down state.</li>
            <li><strong>+new-epoch</strong> <code>&lt;instance details&gt;</code> -- The current epoch was updated.</li>
            <li><strong>+try-failover</strong> <code>&lt;instance details&gt;</code> -- New failover in progress, waiting to be elected by the majority.</li>
            <li><strong>+elected-leader</strong> <code>&lt;instance details&gt;</code> -- Won the election for the specified epoch, can do the failover.</li>
            <li><strong>+failover-state-select-slave</strong> <code>&lt;instance details&gt;</code> -- New failover state is <code>select-slave</code>: we are trying to find a suitable slave for promotion.</li>
            <li><strong>no-good-slave</strong> <code>&lt;instance details&gt;</code> -- There is no good slave to promote. Currently we&#39;ll try after some time, but probably this will change and the state machine will abort the failover at all in this case.</li>
            <li><strong>selected-slave</strong> <code>&lt;instance details&gt;</code> -- We found the specified good slave to promote.</li>
            <li><strong>failover-state-send-slaveof-noone</strong> <code>&lt;instance details&gt;</code> -- We are trying to reconfigure the promoted slave as master, waiting for it to switch.</li>
            <li><strong>failover-end-for-timeout</strong> <code>&lt;instance details&gt;</code> -- The failover terminated for timeout, slaves will eventually be configured to replicate with the new master anyway.</li>
            <li><strong>failover-end</strong> <code>&lt;instance details&gt;</code> -- The failover terminated with success. All the slaves appears to be reconfigured to replicate with the new master.</li>
            <li><strong>switch-master</strong> <code>&lt;master name&gt; &lt;oldip&gt; &lt;oldport&gt; &lt;newip&gt; &lt;newport&gt;</code> -- The master new IP and address is the specified one after a configuration change. This is <strong>the message most external users are interested in</strong>.</li>
            <li><strong>+tilt</strong> -- Tilt mode entered.</li>
            <li><strong>-tilt</strong> -- Tilt mode exited.</li>
            </ul>
            
            <span id="handling-of--busy-state" class=anchor></span><h2 ><a href="#handling-of--busy-state" class=anchor-link>*</a>Handling of -BUSY state</h2>
            
            <p>The -BUSY error is returned by a Redis instance when a Lua script is running for
            more time than the configured Lua script time limit. When this happens before
            triggering a fail over Redis Sentinel will try to send a <a href="/commands/script-kill">SCRIPT KILL</a>
            command, that will only succeed if the script was read-only.</p>
            
            <p>If the instance will still be in an error condition after this try, it will
            eventually be failed over.</p>
            
            <span id="slaves-priority" class=anchor></span><h2 ><a href="#slaves-priority" class=anchor-link>*</a>Slaves priority</h2>
            
            <p>Redis instances have a configuration parameter called <code>slave-priority</code>.
            This information is exposed by Redis slave instances in their <a href="/commands/info">INFO</a> output,
            and Sentinel uses it in order to pick a slave among the ones that can be
            used in order to failover a master:</p>
            
            <ol>
            <li>If the slave priority is set to 0, the slave is never promoted to master.</li>
            <li>Slaves with a <em>lower</em> priority number are preferred by Sentinel.</li>
            </ol>
            
            <p>For example if there is a slave S1 in the same data center of the current
            master, and another slave S2 in another data center, it is possible to set
            S1 with a priority of 10 and S2 with a priority of 100, so that if the master
            fails and both S1 and S2 are available, S1 will be preferred.</p>
            
            <p>For more information about the way slaves are selected, please check the <strong>slave selection and priority</strong> section of this documentation.</p>
            
            <span id="sentinel-and-redis-authentication" class=anchor></span><h2 ><a href="#sentinel-and-redis-authentication" class=anchor-link>*</a>Sentinel and Redis authentication</h2>
            
            <p>When the master is configured to require a password from clients,
            as a security measure, slaves need to also be aware of this password in
            order to authenticate with the master and create the master-slave connection
            used for the asynchronous replication protocol.</p>
            
            <p>This is achieved using the following configuration directives:</p>
            
            <ul>
            <li><code>requirepass</code> in the master, in order to set the authentication password, and to make sure the instance will not process requests for non authenticated clients.</li>
            <li><code>masterauth</code> in the slaves in order for the slaves to authenticate with the master in order to correctly replicate data from it.</li>
            </ul>
            
            <p>When Sentinel is used, there is not a single master, since after a failover
            slaves may play the role of masters, and old masters can be reconfigured in
            order to act as slaves, so what you want to do is to set the above directives
            in all your instances, both masters and slaves.</p>
            
            <p>This is also usually a sane setup since you don&#39;t want to protect
            data only in the master, having the same data accessible in the slaves.</p>
            
            <p>However, in the uncommon case where you need a slave that is accessible
            without authentication, you can still do it by setting up <strong>a slave priority
            of zero</strong>, to prevent this slave from being promoted to master, and
            configuring in this slave only the <code>masterauth</code> directive, without
            using the <code>requirepass</code> directive, so that data will be readable by
            unauthenticated clients.</p>
            
            <p>In order for sentinels to connect to Redis server instances when they are
            configured with <code>requirepass</code>, the Sentinel configuration must include the
            <code>sentinel auth-pass</code> directive, in the format:</p>
            
            <pre><code>sentinel auth-pass &lt;master-group-name&gt; &lt;pass&gt;&#x000A;</code></pre>
            
            <span id="configuring-sentinel-instances-with-authentication" class=anchor></span><h2 ><a href="#configuring-sentinel-instances-with-authentication" class=anchor-link>*</a>Configuring Sentinel instances with authentication</h2>
            
            <p>You can also configure the Sentinel instance itself in order to require
            client authentication via the <a href="/commands/auth">AUTH</a> command, however this feature is
            only available starting with Redis 5.0.1.</p>
            
            <p>In order to do so, just add the following configuration directive to
            all your Sentinel instances:</p>
            
            <pre><code>requirepass &quot;your_password_here&quot;&#x000A;</code></pre>
            
            <p>When configured this way, Sentinels will do two things:</p>
            
            <ol>
            <li>A password will be required from clients in order to send commands to Sentinels. This is obvious since this is how such configuration directive works in Redis in general.</li>
            <li>Moreover the same password configured to access the local Sentinel, will be used by this Sentinel instance in order to authenticate to all the other Sentinel instances it connects to.</li>
            </ol>
            
            <p>This means that <strong>you will have to configure the same <code>requirepass</code> password in all the Sentinel instances</strong>. This way every Sentinel can talk with every other Sentinel without any need to configure for each Sentinel the password to access all the other Sentinels, that would be very impractical.</p>
            
            <p>Before using this configuration make sure your client library is able to send the <a href="/commands/auth">AUTH</a> command to Sentinel instances.</p>
            
            <span id="sentinel-clients-implementation" class=anchor></span><h2 ><a href="#sentinel-clients-implementation" class=anchor-link>*</a>Sentinel clients implementation</h2>
            
            <p>Sentinel requires explicit client support, unless the system is configured to execute a script that performs a transparent redirection of all the requests to the new master instance (virtual IP or other similar systems). The topic of client libraries implementation is covered in the document <a href="/topics/sentinel-clients">Sentinel clients guidelines</a>.</p>
            
            <span id="more-advanced-concepts" class=anchor></span><h1 ><a href="#more-advanced-concepts" class=anchor-link>*</a>More advanced concepts</h1>
            
            <p>In the following sections we&#39;ll cover a few details about how Sentinel work,
            without to resorting to implementation details and algorithms that will be
            covered in the final part of this document.</p>
            
            <span id="sdown-and-odown-failure-state" class=anchor></span><h2 ><a href="#sdown-and-odown-failure-state" class=anchor-link>*</a>SDOWN and ODOWN failure state</h2>
            
            <p>Redis Sentinel has two different concepts of <em>being down</em>, one is called
            a <em>Subjectively Down</em> condition (SDOWN) and is a down condition that is
            local to a given Sentinel instance. Another is called <em>Objectively Down</em>
            condition (ODOWN) and is reached when enough Sentinels (at least the
            number configured as the <code>quorum</code> parameter of the monitored master) have
            an SDOWN condition, and get feedback from other Sentinels using
            the <code>SENTINEL is-master-down-by-addr</code> command.</p>
            
            <p>From the point of view of a Sentinel an SDOWN condition is reached when it
            does not receive a valid reply to PING requests for the number of seconds
            specified in the configuration as <code>is-master-down-after-milliseconds</code>
            parameter.</p>
            
            <p>An acceptable reply to PING is one of the following:</p>
            
            <ul>
            <li>PING replied with +PONG.</li>
            <li>PING replied with -LOADING error.</li>
            <li>PING replied with -MASTERDOWN error.</li>
            </ul>
            
            <p>Any other reply (or no reply at all) is considered non valid.
            However note that <strong>a logical master that advertises itself as a slave in
            the INFO output is considered to be down</strong>.</p>
            
            <p>Note that SDOWN requires that no acceptable reply is received for the whole
            interval configured, so for instance if the interval is 30000 milliseconds
            (30 seconds) and we receive an acceptable ping reply every 29 seconds, the
            instance is considered to be working.</p>
            
            <p>SDOWN is not enough to trigger a failover: it only means a single Sentinel
            believes a Redis instance is not available. To trigger a failover, the
            ODOWN state must be reached.</p>
            
            <p>To switch from SDOWN to ODOWN no strong consensus algorithm is used, but
            just a form of gossip: if a given Sentinel gets reports that a master
            is not working from enough Sentinels <strong>in a given time range</strong>, the SDOWN is
            promoted to ODOWN. If this acknowledge is later missing, the flag is cleared.</p>
            
            <p>A more strict authorization that uses an actual majority is required in
            order to really start the failover, but no failover can be triggered without
            reaching the ODOWN state.</p>
            
            <p>The ODOWN condition <strong>only applies to masters</strong>. For other kind of instances
            Sentinel doesn&#39;t require to act, so the ODOWN state is never reached for slaves
            and other sentinels, but only SDOWN is.</p>
            
            <p>However SDOWN has also semantic implications. For example a slave in SDOWN
            state is not selected to be promoted by a Sentinel performing a failover.</p>
            
            <span id="sentinels-and-slaves-auto-discovery" class=anchor></span><h2 ><a href="#sentinels-and-slaves-auto-discovery" class=anchor-link>*</a>Sentinels and Slaves auto discovery</h2>
            
            <p>Sentinels stay connected with other Sentinels in order to reciprocally
            check the availability of each other, and to exchange messages. However you
            don&#39;t need to configure a list of other Sentinel addresses in every Sentinel
            instance you run, as Sentinel uses the Redis instances Pub/Sub capabilities
            in order to discover the other Sentinels that are monitoring the same masters
            and slaves.</p>
            
            <p>This feature is implemented by sending <em>hello messages</em> into the channel named
            <code>__sentinel__:hello</code>.</p>
            
            <p>Similarly you don&#39;t need to configure what is the list of the slaves attached
            to a master, as Sentinel will auto discover this list querying Redis.</p>
            
            <ul>
            <li>Every Sentinel publishes a message to every monitored master and slave Pub/Sub channel <code>__sentinel__:hello</code>, every two seconds, announcing its presence with ip, port, runid.</li>
            <li>Every Sentinel is subscribed to the Pub/Sub channel <code>__sentinel__:hello</code> of every master and slave, looking for unknown sentinels. When new sentinels are detected, they are added as sentinels of this master.</li>
            <li>Hello messages also include the full current configuration of the master. If the receiving Sentinel has a configuration for a given master which is older than the one received, it updates to the new configuration immediately.</li>
            <li>Before adding a new sentinel to a master a Sentinel always checks if there is already a sentinel with the same runid or the same address (ip and port pair). In that case all the matching sentinels are removed, and the new added.</li>
            </ul>
            
            <span id="sentinel-reconfiguration-of-instances-outside-the-failover-procedure" class=anchor></span><h2 ><a href="#sentinel-reconfiguration-of-instances-outside-the-failover-procedure" class=anchor-link>*</a>Sentinel reconfiguration of instances outside the failover procedure</h2>
            
            <p>Even when no failover is in progress, Sentinels will always try to set the
            current configuration on monitored instances. Specifically:</p>
            
            <ul>
            <li>Slaves (according to the current configuration) that claim to be masters, will be configured as slaves to replicate with the current master.</li>
            <li>Slaves connected to a wrong master, will be reconfigured to replicate with the right master.</li>
            </ul>
            
            <p>For Sentinels to reconfigure slaves, the wrong configuration must be observed for some time, that is greater than the period used to broadcast new configurations.</p>
            
            <p>This prevents Sentinels with a stale configuration (for example because they just rejoined from a partition) will try to change the slaves configuration before receiving an update.</p>
            
            <p>Also note how the semantics of always trying to impose the current configuration makes the failover more resistant to partitions:</p>
            
            <ul>
            <li>Masters failed over are reconfigured as slaves when they return available.</li>
            <li>Slaves partitioned away during a partition are reconfigured once reachable.</li>
            </ul>
            
            <p>The important lesson to remember about this section is: <strong>Sentinel is a system where each process will always try to impose the last logical configuration to the set of monitored instances</strong>.</p>
            
            <span id="slave-selection-and-priority" class=anchor></span><h2 ><a href="#slave-selection-and-priority" class=anchor-link>*</a>Slave selection and priority</h2>
            
            <p>When a Sentinel instance is ready to perform a failover, since the master
            is in <code>ODOWN</code> state and the Sentinel received the authorization to failover
            from the majority of the Sentinel instances known, a suitable slave needs
            to be selected.</p>
            
            <p>The slave selection process evaluates the following information about slaves:</p>
            
            <ol>
            <li>Disconnection time from the master.</li>
            <li>Slave priority.</li>
            <li>Replication offset processed.</li>
            <li>Run ID.</li>
            </ol>
            
            <p>A slave that is found to be disconnected from the master for more than ten
            times the configured master timeout (down-after-milliseconds option), plus
            the time the master is also not available from the point of view of the
            Sentinel doing the failover, is considered to be not suitable for the failover
            and is skipped.</p>
            
            <p>In more rigorous terms, a slave whose the <a href="/commands/info">INFO</a> output suggests to be
            disconnected from the master for more than:</p>
            
            <pre><code>(down-after-milliseconds * 10) + milliseconds_since_master_is_in_SDOWN_state&#x000A;</code></pre>
            
            <p>Is considered to be unreliable and is disregarded entirely.</p>
            
            <p>The slave selection only considers the slaves that passed the above test,
            and sorts it based on the above criteria, in the following order.</p>
            
            <ol>
            <li>The slaves are sorted by <code>slave-priority</code> as configured in the <code>redis.conf</code> file of the Redis instance. A lower priority will be preferred.</li>
            <li>If the priority is the same, the replication offset processed by the slave is checked, and the slave that received more data from the master is selected.</li>
            <li>If multiple slaves have the same priority and processed the same data from the master, a further check is performed, selecting the slave with the lexicographically smaller run ID. Having a lower run ID is not a real advantage for a slave, but is useful in order to make the process of slave selection more deterministic, instead of resorting to select a random slave.</li>
            </ol>
            
            <p>Redis masters (that may be turned into slaves after a failover), and slaves, all
            must be configured with a <code>slave-priority</code> if there are machines to be strongly
            preferred. Otherwise all the instances can run with the default run ID (which
            is the suggested setup, since it is far more interesting to select the slave
            by replication offset).</p>
            
            <p>A Redis instance can be configured with a special <code>slave-priority</code> of zero
            in order to be <strong>never selected</strong> by Sentinels as the new master.
            However a slave configured in this way will still be reconfigured by
            Sentinels in order to replicate with the new master after a failover, the
            only difference is that it will never become a master itself.</p>
            
            <span id="algorithms-and-internals" class=anchor></span><h1 ><a href="#algorithms-and-internals" class=anchor-link>*</a>Algorithms and internals</h1>
            
            <p>In the following sections we will explore the details of Sentinel behavior.
            It is not strictly needed for users to be aware of all the details, but a
            deep understanding of Sentinel may help to deploy and operate Sentinel in
            a more effective way.</p>
            
            <span id="quorum" class=anchor></span><h2 ><a href="#quorum" class=anchor-link>*</a>Quorum</h2>
            
            <p>The previous sections showed that every master monitored by Sentinel is associated to a configured <strong>quorum</strong>. It specifies the number of Sentinel processes
            that need to agree about the unreachability or error condition of the master in
            order to trigger a failover.</p>
            
            <p>However, after the failover is triggered, in order for the failover to actually be performed, <strong>at least a majority of Sentinels must authorize the Sentinel to
            failover</strong>. Sentinel never performs a failover in the partition where a
            minority of Sentinels exist.</p>
            
            <p>Let&#39;s try to make things a bit more clear:</p>
            
            <ul>
            <li>Quorum: the number of Sentinel processes that need to detect an error condition in order for a master to be flagged as <strong>ODOWN</strong>.</li>
            <li>The failover is triggered by the <strong>ODOWN</strong> state.</li>
            <li>Once the failover is triggered, the Sentinel trying to failover is required to ask for authorization to a majority of Sentinels (or more than the majority if the quorum is set to a number greater than the majority).</li>
            </ul>
            
            <p>The difference may seem subtle but is actually quite simple to understand and use.  For example if you have 5 Sentinel instances, and the quorum is set to 2, a failover will be triggered as soon as 2 Sentinels believe that the master is not reachable, however one of the two Sentinels will be able to failover only if it gets authorization at least from 3 Sentinels.</p>
            
            <p>If instead the quorum is configured to 5, all the Sentinels must agree about the master error condition, and the authorization from all Sentinels is required in order to failover.</p>
            
            <p>This means that the quorum can be used to tune Sentinel in two ways:</p>
            
            <ol>
            <li>If a the quorum is set to a value smaller than the majority of Sentinels we deploy, we are basically making Sentinel more sensible to master failures, triggering a failover as soon as even just a minority of Sentinels is no longer able to talk with the master.</li>
            <li>If a quorum is set to a value greater than the majority of Sentinels, we are making Sentinel able to failover only when there are a very large number (larger than majority) of well connected Sentinels which agree about the master being down.</li>
            </ol>
            
            <span id="configuration-epochs" class=anchor></span><h2 ><a href="#configuration-epochs" class=anchor-link>*</a>Configuration epochs</h2>
            
            <p>Sentinels require to get authorizations from a majority in order to start a
            failover for a few important reasons:</p>
            
            <p>When a Sentinel is authorized, it gets a unique <strong>configuration epoch</strong> for the master it is failing over. This is a number that will be used to version the new configuration after the failover is completed. Because a majority agreed that a given version was assigned to a given Sentinel, no other Sentinel will be able to use it. This means that every configuration of every failover is versioned with a unique version. We&#39;ll see why this is so important.</p>
            
            <p>Moreover Sentinels have a rule: if a Sentinel voted another Sentinel for the failover of a given master, it will wait some time to try to failover the same master again. This delay is the <code>failover-timeout</code> you can configure in <code>sentinel.conf</code>. This means that Sentinels will not try to failover the same master at the same time, the first to ask to be authorized will try, if it fails another will try after some time, and so forth.</p>
            
            <p>Redis Sentinel guarantees the <em>liveness</em> property that if a majority of Sentinels are able to talk, eventually one will be authorized to failover if the master is down.</p>
            
            <p>Redis Sentinel also guarantees the <em>safety</em> property that every Sentinel will failover the same master using a different <em>configuration epoch</em>.</p>
            
            <span id="configuration-propagation" class=anchor></span><h2 ><a href="#configuration-propagation" class=anchor-link>*</a>Configuration propagation</h2>
            
            <p>Once a Sentinel is able to failover a master successfully, it will start to broadcast the new configuration so that the other Sentinels will update their information about a given master.</p>
            
            <p>For a failover to be considered successful, it requires that the Sentinel was able to send the <code>SLAVEOF NO ONE</code> command to the selected slave, and that the switch to master was later observed in the <a href="/commands/info">INFO</a> output of the master.</p>
            
            <p>At this point, even if the reconfiguration of the slaves is in progress, the failover is considered to be successful, and all the Sentinels are required to start reporting the new configuration.</p>
            
            <p>The way a new configuration is propagated is the reason why we need that every
            Sentinel failover is authorized with a different version number (configuration epoch).</p>
            
            <p>Every Sentinel continuously broadcast its version of the configuration of a master using Redis Pub/Sub messages, both in the master and all the slaves.  At the same time all the Sentinels wait for messages to see what is the configuration
            advertised by the other Sentinels.</p>
            
            <p>Configurations are broadcast in the <code>__sentinel__:hello</code> Pub/Sub channel.</p>
            
            <p>Because every configuration has a different version number, the greater version
            always wins over smaller versions.</p>
            
            <p>So for example the configuration for the master <code>mymaster</code> start with all the
            Sentinels believing the master is at 192.168.1.50:6379. This configuration
            has version 1. After some time a Sentinel is authorized to failover with version 2. If the failover is successful, it will start to broadcast a new configuration, let&#39;s say 192.168.1.50:9000, with version 2. All the other instances will see this configuration and will update their configuration accordingly, since the new configuration has a greater version.</p>
            
            <p>This means that Sentinel guarantees a second liveness property: a set of
            Sentinels that are able to communicate will all converge to the same configuration with the higher version number.</p>
            
            <p>Basically if the net is partitioned, every partition will converge to the higher
            local configuration. In the special case of no partitions, there is a single
            partition and every Sentinel will agree about the configuration.</p>
            
            <span id="consistency-under-partitions" class=anchor></span><h2 ><a href="#consistency-under-partitions" class=anchor-link>*</a>Consistency under partitions</h2>
            
            <p>Redis Sentinel configurations are eventually consistent, so every partition will
            converge to the higher configuration available.
            However in a real-world system using Sentinel there are three different players:</p>
            
            <ul>
            <li>Redis instances.</li>
            <li>Sentinel instances.</li>
            <li>Clients.</li>
            </ul>
            
            <p>In order to define the behavior of the system we have to consider all three.</p>
            
            <p>The following is a simple network where there are 3 nodes, each running
            a Redis instance, and a Sentinel instance:</p>
            
            <pre><code>            +-------------+&#x000A;            | Sentinel 1  |----- Client A&#x000A;            | Redis 1 (M) |&#x000A;            +-------------+&#x000A;                    |&#x000A;                    |&#x000A;+-------------+     |          +------------+&#x000A;| Sentinel 2  |-----+-- // ----| Sentinel 3 |----- Client B&#x000A;| Redis 2 (S) |                | Redis 3 (M)|&#x000A;+-------------+                +------------+&#x000A;</code></pre>
            
            <p>In this system the original state was that Redis 3 was the master, while
            Redis 1 and 2 were slaves. A partition occurred isolating the old master.
            Sentinels 1 and 2 started a failover promoting Sentinel 1 as the new master.</p>
            
            <p>The Sentinel properties guarantee that Sentinel 1 and 2 now have the new
            configuration for the master. However Sentinel 3 has still the old configuration
            since it lives in a different partition.</p>
            
            <p>We know that Sentinel 3 will get its configuration updated when the network
            partition will heal, however what happens during the partition if there
            are clients partitioned with the old master?</p>
            
            <p>Clients will be still able to write to Redis 3, the old master. When the
            partition will rejoin, Redis 3 will be turned into a slave of Redis 1, and
            all the data written during the partition will be lost.</p>
            
            <p>Depending on your configuration you may want or not that this scenario happens:</p>
            
            <ul>
            <li>If you are using Redis as a cache, it could be handy that Client B is still able to write to the old master, even if its data will be lost.</li>
            <li>If you are using Redis as a store, this is not good and you need to configure the system in order to partially prevent this problem.</li>
            </ul>
            
            <p>Since Redis is asynchronously replicated, there is no way to totally prevent data loss in this scenario, however you can bound the divergence between Redis 3 and Redis 1
            using the following Redis configuration option:</p>
            
            <pre><code>min-slaves-to-write 1&#x000A;min-slaves-max-lag 10&#x000A;</code></pre>
            
            <p>With the above configuration (please see the self-commented <code>redis.conf</code> example in the Redis distribution for more information) a Redis instance, when acting as a master, will stop accepting writes if it can&#39;t write to at least 1 slave. Since replication is asynchronous <em>not being able to write</em> actually means that the slave is either disconnected, or is not sending us asynchronous acknowledges for more than the specified <code>max-lag</code> number of seconds.</p>
            
            <p>Using this configuration the Redis 3 in the above example will become unavailable after 10 seconds. When the partition heals, the Sentinel 3 configuration will converge to
            the new one, and Client B will be able to fetch a valid configuration and continue.</p>
            
            <p>In general Redis + Sentinel as a whole are a an <strong>eventually consistent system</strong> where the merge function is <strong>last failover wins</strong>, and the data from old masters are discarded to replicate the data of the current master, so there is always a window for losing acknowledged writes. This is due to Redis asynchronous
            replication and the discarding nature of the &quot;virtual&quot; merge function of the system. Note that this is not a limitation of Sentinel itself, and if you orchestrate the failover with a strongly consistent replicated state machine, the same properties will still apply. There are only two ways to avoid losing acknowledged writes:</p>
            
            <ol>
            <li>Use synchronous replication (and a proper consensus algorithm to run a replicated state machine).</li>
            <li>Use an eventually consistent system where different versions of the same object can be merged.</li>
            </ol>
            
            <p>Redis currently is not able to use any of the above systems, and is currently outside the development goals. However there are proxies implementing solution &quot;2&quot; on top of Redis stores such as SoundCloud <a href="https://github.com/soundcloud/roshi">Roshi</a>, or Netflix <a href="https://github.com/Netflix/dynomite">Dynomite</a>.</p>
            
            <span id="sentinel-persistent-state" class=anchor></span><h2 ><a href="#sentinel-persistent-state" class=anchor-link>*</a>Sentinel persistent state</h2>
            
            <p>Sentinel state is persisted in the sentinel configuration file. For example
            every time a new configuration is received, or created (leader Sentinels), for
            a master, the configuration is persisted on disk together with the configuration
            epoch. This means that it is safe to stop and restart Sentinel processes.</p>
            
            <span id="tilt-mode" class=anchor></span><h2 ><a href="#tilt-mode" class=anchor-link>*</a>TILT mode</h2>
            
            <p>Redis Sentinel is heavily dependent on the computer time: for instance in
            order to understand if an instance is available it remembers the time of the
            latest successful reply to the PING command, and compares it with the current
            time to understand how old it is.</p>
            
            <p>However if the computer time changes in an unexpected way, or if the computer
            is very busy, or the process blocked for some reason, Sentinel may start to
            behave in an unexpected way.</p>
            
            <p>The TILT mode is a special &quot;protection&quot; mode that a Sentinel can enter when
            something odd is detected that can lower the reliability of the system.
            The Sentinel timer interrupt is normally called 10 times per second, so we
            expect that more or less 100 milliseconds will elapse between two calls
            to the timer interrupt.</p>
            
            <p>What a Sentinel does is to register the previous time the timer interrupt
            was called, and compare it with the current call: if the time difference
            is negative or unexpectedly big (2 seconds or more) the TILT mode is entered
            (or if it was already entered the exit from the TILT mode postponed).</p>
            
            <p>When in TILT mode the Sentinel will continue to monitor everything, but:</p>
            
            <ul>
            <li>It stops acting at all.</li>
            <li>It starts to reply negatively to <code>SENTINEL is-master-down-by-addr</code> requests as the ability to detect a failure is no longer trusted.</li>
            </ul>
            
            <p>If everything appears to be normal for 30 second, the TILT mode is exited.</p>
            
            <p>Note that in some way TILT mode could be replaced using the monotonic clock
            API that many kernels offer. However it is not still clear if this is a good
            solution since the current system avoids issues in case the process is just
            suspended or not executed by the scheduler for a long time.</p>
          </article>
        </div>
      </div>
      <footer class='site-footer'>
        <div class='container'>
          <p>
            This website is
            <a href="https://github.com/antirez/redis-io">open source software</a>.
            See all <a href="/topics/sponsors">credits</a>.
          </p>
          <div class='sponsor'>
            Sponsored by
            <a href='https://redislabs.com/'>
              <img alt='Redis Labs' height='25' src='/images/redislabs.png' title='Get a Managed Redis' width='128'>
            </a>
          </div>
        </div>
      </footer>
    </div>
    <script src='https://ajax.googleapis.com/ajax/libs/jquery/1.4/jquery.min.js'></script>
    <script src='/app.js?1480208557'></script>
  </body>
</html>
