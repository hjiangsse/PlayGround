<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <title>Redis cluster tutorial â€“ Redis</title>
    <link href='/styles.css' rel='stylesheet'>
    <link href='/images/favicon.png' rel='shortcut icon'>
    <link href='/opensearch.xml' rel='search' title='Look up a Redis command' type='application/opensearchdescription+xml'>
    <meta content='width=device-width, minimum-scale=1.0, maximum-scale=1.0' name='viewport'>
    <script>
       var _gaq = _gaq || [];
       _gaq.push(['_setAccount', 'UA-20243082-1']);
       _gaq.push(['_trackPageview']);
      
       (function() {
         var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
         ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
         var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
       })();
    </script>
  </head>
  <body class='topics cluster-tutorial'>
    <div class='mobile-menu slideout-menu'>
      <header class='menu-header'></header>
      <section class='menu-section'>
        <ul class='menu-section-list'>
          <li>
            <a class='home' href='/'>Home</a>
          </li>
          <li>
            <a href='/commands'>Commands</a>
          </li>
          <li>
            <a href='/clients'>Clients</a>
          </li>
          <li>
            <a href='/documentation'>Documentation</a>
          </li>
          <li>
            <a href='/community'>Community</a>
          </li>
          <li>
            <a href='/download'>Download</a>
          </li>
          <li>
            <a href='/modules'>Modules</a>
          </li>
          <li>
            <a href='/support'>Support</a>
          </li>
        </ul>
      </section>
    </div>
    <div class='site-wrapper'>
      <header class='site-header'>
        <nav class='container'>
          <div class='mobile-header'>
            <button class='btn-hamburger js-slideout-toggle'>
              <span class='fa fa-bars'></span>
            </button>
            <a class='home' href='/'>
              <img alt='Redis' src='/images/redis-white.png'>
            </a>
          </div>
          <div class='desktop-header'>
            <a class='home' href='/'>
              <img alt='Redis' src='/images/redis-white.png'>
            </a>
            <a href='/commands'>Commands</a>
            <a href='/clients'>Clients</a>
            <a href='/documentation'>Documentation</a>
            <a href='/community'>Community</a>
            <a href='/download'>Download</a>
            <a href='/modules'>Modules</a>
            <a href='/support'>Support</a>
          </div>
        </nav>
      </header>
      <div class='site-content'>
        <div class='text'>
          <article id='topic'>
            <span id="redis-cluster-tutorial" class=anchor></span><h1 ><a href="#redis-cluster-tutorial" class=anchor-link>*</a>Redis cluster tutorial</h1>
            
            <p>This document is a gentle introduction to Redis Cluster, that does not use
            complex to understand distributed systems concepts. It provides instructions
            about how to setup a cluster, test, and operate it, without
            going into the details that are covered in
            the <a href="/topics/cluster-spec">Redis Cluster specification</a> but just describing
            how the system behaves from the point of view of the user.</p>
            
            <p>However this tutorial tries to provide information about the availability
            and consistency characteristics of Redis Cluster from the point of view
            of the final user, stated in a simple to understand way.</p>
            
            <p>Note this tutorial requires Redis version 3.0 or higher.</p>
            
            <p>If you plan to run a serious Redis Cluster deployment, the
            more formal specification is a suggested reading, even if not
            strictly required. However it is a good idea to start from this document,
            play with Redis Cluster some time, and only later read the specification.</p>
            
            <span id="redis-cluster-101" class=anchor></span><h2 ><a href="#redis-cluster-101" class=anchor-link>*</a>Redis Cluster 101</h2>
            
            <p>Redis Cluster provides a way to run a Redis installation where data is
            <strong>automatically sharded across multiple Redis nodes</strong>.</p>
            
            <p>Redis Cluster also provides <strong>some degree of availability during partitions</strong>,
            that is in practical terms the ability to continue the operations when
            some nodes fail or are not able to communicate. However the cluster stops
            to operate in the event of larger failures (for example when the majority of
            masters are unavailable).</p>
            
            <p>So in practical terms, what do you get with Redis Cluster?</p>
            
            <ul>
            <li>The ability to <strong>automatically split your dataset among multiple nodes</strong>.</li>
            <li>The ability to <strong>continue operations when a subset of the nodes are experiencing failures</strong> or are unable to communicate with the rest of the cluster.</li>
            </ul>
            
            <span id="redis-cluster-tcp-ports" class=anchor></span><h2 ><a href="#redis-cluster-tcp-ports" class=anchor-link>*</a>Redis Cluster TCP ports</h2>
            
            <p>Every Redis Cluster node requires two TCP connections open. The normal Redis
            TCP port used to serve clients, for example 6379, plus the port obtained by
            adding 10000 to the data port, so 16379 in the example.</p>
            
            <p>This second <em>high</em> port is used for the Cluster bus, that is a node-to-node
            communication channel using a binary protocol. The Cluster bus is used by
            nodes for failure detection, configuration update, failover authorization
            and so forth. Clients should never try to communicate with the cluster bus
            port, but always with the normal Redis command port, however make sure you
            open both ports in your firewall, otherwise Redis cluster nodes will be
            not able to communicate.</p>
            
            <p>The command port and cluster bus port offset is fixed and is always 10000.</p>
            
            <p>Note that for a Redis Cluster to work properly you need, for each node:</p>
            
            <ol>
            <li>The normal client communication port (usually 6379) used to communicate with clients to be open to all the clients that need to reach the cluster, plus all the other cluster nodes (that use the client port for keys migrations).</li>
            <li>The cluster bus port (the client port + 10000) must be reachable from all the other cluster nodes.</li>
            </ol>
            
            <p>If you don&#39;t open both TCP ports, your cluster will not work as expected.</p>
            
            <p>The cluster bus uses a different, binary protocol, for node to node data
            exchange, which is more suited to exchange information between nodes using
            little bandwidth and processing time.</p>
            
            <span id="redis-cluster-and-docker" class=anchor></span><h2 ><a href="#redis-cluster-and-docker" class=anchor-link>*</a>Redis Cluster and Docker</h2>
            
            <p>Currently Redis Cluster does not support NATted environments and in general
            environments where IP addresses or TCP ports are remapped.</p>
            
            <p>Docker uses a technique called <em>port mapping</em>: programs running inside Docker
            containers may be exposed with a different port compared to the one the
            program believes to be using. This is useful in order to run multiple
            containers using the same ports, at the same time, in the same server.</p>
            
            <p>In order to make Docker compatible with Redis Cluster you need to use
            the <strong>host networking mode</strong> of Docker. Please check the <code>--net=host</code> option
            in the <a href="https://docs.docker.com/engine/userguide/networking/dockernetworks/">Docker documentation</a> for more information.</p>
            
            <span id="redis-cluster-data-sharding" class=anchor></span><h2 ><a href="#redis-cluster-data-sharding" class=anchor-link>*</a>Redis Cluster data sharding</h2>
            
            <p>Redis Cluster does not use consistent hashing, but a different form of sharding
            where every key is conceptually part of what we call an <strong>hash slot</strong>.</p>
            
            <p>There are 16384 hash slots in Redis Cluster, and to compute what is the hash
            slot of a given key, we simply take the CRC16 of the key modulo
            16384.</p>
            
            <p>Every node in a Redis Cluster is responsible for a subset of the hash slots,
            so for example you may have a cluster with 3 nodes, where:</p>
            
            <ul>
            <li>Node A contains hash slots from 0 to 5500.</li>
            <li>Node B contains hash slots from 5501 to 11000.</li>
            <li>Node C contains hash slots from 11001 to 16383.</li>
            </ul>
            
            <p>This allows to add and remove nodes in the cluster easily. For example if
            I want to add a new node D, I need to move some hash slot from nodes A, B, C
            to D. Similarly if I want to remove node A from the cluster I can just
            move the hash slots served by A to B and C. When the node A will be empty
            I can remove it from the cluster completely.</p>
            
            <p>Because moving hash slots from a node to another does not require to stop
            operations, adding and removing nodes, or changing the percentage of hash
            slots hold by nodes, does not require any downtime.</p>
            
            <p>Redis Cluster supports multiple key operations as long as all the keys involved
            into a single command execution (or whole transaction, or Lua script
            execution) all belong to the same hash slot. The user can force multiple keys
            to be part of the same hash slot by using a concept called <em>hash tags</em>.</p>
            
            <p>Hash tags are documented in the Redis Cluster specification, but the gist is
            that if there is a substring between {} brackets in a key, only what is
            inside the string is hashed, so for example <code>this{foo}key</code> and <code>another{foo}key</code>
            are guaranteed to be in the same hash slot, and can be used together in a
            command with multiple keys as arguments.</p>
            
            <span id="redis-cluster-master-slave-model" class=anchor></span><h2 ><a href="#redis-cluster-master-slave-model" class=anchor-link>*</a>Redis Cluster master-slave model</h2>
            
            <p>In order to remain available when a subset of master nodes are failing or are
            not able to communicate with the majority of nodes, Redis Cluster uses a
            master-slave model where every hash slot has from 1 (the master itself) to N
            replicas (N-1 additional slaves nodes).</p>
            
            <p>In our example cluster with nodes A, B, C, if node B fails the cluster is not
            able to continue, since we no longer have a way to serve hash slots in the
            range 5501-11000.</p>
            
            <p>However when the cluster is created (or at a later time) we add a slave
            node to every master, so that the final cluster is composed of A, B, C
            that are masters nodes, and A1, B1, C1 that are slaves nodes, the system is
            able to continue if node B fails.</p>
            
            <p>Node B1 replicates B, and B fails, the cluster will promote node B1 as the new
            master and will continue to operate correctly.</p>
            
            <p>However note that if nodes B and B1 fail at the same time Redis Cluster is not
            able to continue to operate.</p>
            
            <span id="redis-cluster-consistency-guarantees" class=anchor></span><h2 ><a href="#redis-cluster-consistency-guarantees" class=anchor-link>*</a>Redis Cluster consistency guarantees</h2>
            
            <p>Redis Cluster is not able to guarantee <strong>strong consistency</strong>. In practical
            terms this means that under certain conditions it is possible that Redis
            Cluster will lose writes that were acknowledged by the system to the client.</p>
            
            <p>The first reason why Redis Cluster can lose writes is because it uses
            asynchronous replication. This means that during writes the following
            happens:</p>
            
            <ul>
            <li>Your client writes to the master B.</li>
            <li>The master B replies OK to your client.</li>
            <li>The master B propagates the write to its slaves B1, B2 and B3.</li>
            </ul>
            
            <p>As you can see B does not wait for an acknowledge from B1, B2, B3 before
            replying to the client, since this would be a prohibitive latency penalty
            for Redis, so if your client writes something, B acknowledges the write,
            but crashes before being able to send the write to its slaves, one of the
            slaves (that did not receive the write) can be promoted to master, losing
            the write forever.</p>
            
            <p>This is <strong>very similar to what happens</strong> with most databases that are
            configured to flush data to disk every second, so it is a scenario you
            are already able to reason about because of past experiences with traditional
            database systems not involving distributed systems. Similarly you can
            improve consistency by forcing the database to flush data on disk before
            replying to the client, but this usually results into prohibitively low
            performance. That would be the equivalent of synchronous replication in
            the case of Redis Cluster.</p>
            
            <p>Basically there is a trade-off to take between performance and consistency.</p>
            
            <p>Redis Cluster has support for synchronous writes when absolutely needed,
            implemented via the <a href="/commands/wait">WAIT</a> command, this makes losing writes a lot less
            likely, however note that Redis Cluster does not implement strong consistency
            even when synchronous replication is used: it is always possible under more
            complex failure scenarios that a slave that was not able to receive the write
            is elected as master.</p>
            
            <p>There is another notable scenario where Redis Cluster will lose writes, that
            happens during a network partition where a client is isolated with a minority
            of instances including at least a master.</p>
            
            <p>Take as an example our 6 nodes cluster composed of A, B, C, A1, B1, C1,
            with 3 masters and 3 slaves. There is also a client, that we will call Z1.</p>
            
            <p>After a partition occurs, it is possible that in one side of the
            partition we have A, C, A1, B1, C1, and in the other side we have B and Z1.</p>
            
            <p>Z1 is still able to write to B, that will accept its writes. If the
            partition heals in a very short time, the cluster will continue normally.
            However if the partition lasts enough time for B1 to be promoted to master
            in the majority side of the partition, the writes that Z1 is sending to B
            will be lost.</p>
            
            <p>Note that there is a <strong>maximum window</strong> to the amount of writes Z1 will be able
            to send to B: if enough time has elapsed for the majority side of the
            partition to elect a slave as master, every master node in the minority
            side stops accepting writes.</p>
            
            <p>This amount of time is a very important configuration directive of Redis
            Cluster, and is called the <strong>node timeout</strong>.</p>
            
            <p>After node timeout has elapsed, a master node is considered to be failing,
            and can be replaced by one of its replicas.
            Similarly after node timeout has elapsed without a master node to be able
            to sense the majority of the other master nodes, it enters an error state
            and stops accepting writes.</p>
            
            <span id="redis-cluster-configuration-parameters" class=anchor></span><h1 ><a href="#redis-cluster-configuration-parameters" class=anchor-link>*</a>Redis Cluster configuration parameters</h1>
            
            <p>We are about to create an example cluster deployment. Before we continue,
            let&#39;s introduce the configuration parameters that Redis Cluster introduces
            in the <code>redis.conf</code> file. Some will be obvious, others will be more clear
            as you continue reading.</p>
            
            <ul>
            <li><strong>cluster-enabled <code>&lt;yes/no&gt;</code></strong>: If yes enables Redis Cluster support in a specific Redis instance. Otherwise the instance starts as a stand alone instance as usual.</li>
            <li><strong>cluster-config-file <code>&lt;filename&gt;</code></strong>: Note that despite the name of this option, this is not an user editable configuration file, but the file where a Redis Cluster node automatically persists the cluster configuration (the state, basically) every time there is a change, in order to be able to re-read it at startup. The file lists things like the other nodes in the cluster, their state, persistent variables, and so forth. Often this file is rewritten and flushed on disk as a result of some message reception.</li>
            <li><strong>cluster-node-timeout <code>&lt;milliseconds&gt;</code></strong>: The maximum amount of time a Redis Cluster node can be unavailable, without it being considered as failing. If a master node is not reachable for more than the specified amount of time, it will be failed over by its slaves. This parameter controls other important things in Redis Cluster. Notably, every node that can&#39;t reach the majority of master nodes for the specified amount of time, will stop accepting queries.</li>
            <li><strong>cluster-slave-validity-factor <code>&lt;factor&gt;</code></strong>: If set to zero, a slave will always try to failover a master, regardless of the amount of time the link between the master and the slave remained disconnected. If the value is positive, a maximum disconnection time is calculated as the <em>node timeout</em> value multiplied by the factor provided with this option, and if the node is a slave, it will not try to start a failover if the master link was disconnected for more than the specified amount of time. For example if the node timeout is set to 5 seconds, and the validity factor is set to 10, a slave disconnected from the master for more than 50 seconds will not try to failover its master. Note that any value different than zero may result in Redis Cluster to be unavailable after a master failure if there is no slave able to failover it. In that case the cluster will return back available only when the original master rejoins the cluster.</li>
            <li><strong>cluster-migration-barrier <code>&lt;count&gt;</code></strong>: Minimum number of slaves a master will remain connected with, for another slave to migrate to a master which is no longer covered by any slave. See the appropriate section about replica migration in this tutorial for more information.</li>
            <li><strong>cluster-require-full-coverage <code>&lt;yes/no&gt;</code></strong>: If this is set to yes, as it is by default, the cluster stops accepting writes if some percentage of the key space is not covered by any node. If the option is set to no, the cluster will still serve queries even if only requests about a subset of keys can be processed.</li>
            </ul>
            
            <span id="creating-and-using-a-redis-cluster" class=anchor></span><h1 ><a href="#creating-and-using-a-redis-cluster" class=anchor-link>*</a>Creating and using a Redis Cluster</h1>
            
            <p>Note: to deploy a Redis Cluster manually it is <strong>very important to learn</strong> certain
            operational aspects of it. However if you want to get a cluster up and running
            ASAP (As Soon As Possible) skip this section and the next one and go directly to <strong>Creating a Redis Cluster using the create-cluster script</strong>.</p>
            
            <p>To create a cluster, the first thing we need is to have a few empty
            Redis instances running in <strong>cluster mode</strong>. This basically means that
            clusters are not created using normal Redis instances as a special mode
            needs to be configured so that the Redis instance will enable the Cluster
            specific features and commands.</p>
            
            <p>The following is a minimal Redis cluster configuration file:</p>
            
            <pre><code>port 7000&#x000A;cluster-enabled yes&#x000A;cluster-config-file nodes.conf&#x000A;cluster-node-timeout 5000&#x000A;appendonly yes&#x000A;</code></pre>
            
            <p>As you can see what enables the cluster mode is simply the <code>cluster-enabled</code>
            directive. Every instance also contains the path of a file where the
            configuration for this node is stored, which by default is <code>nodes.conf</code>.
            This file is never touched by humans; it is simply generated at startup
            by the Redis Cluster instances, and updated every time it is needed.</p>
            
            <p>Note that the <strong>minimal cluster</strong> that works as expected requires to contain
            at least three master nodes. For your first tests it is strongly suggested
            to start a six nodes cluster with three masters and three slaves.</p>
            
            <p>To do so, enter a new directory, and create the following directories named
            after the port number of the instance we&#39;ll run inside any given directory.</p>
            
            <p>Something like:</p>
            
            <pre><code>mkdir cluster-test&#x000A;cd cluster-test&#x000A;mkdir 7000 7001 7002 7003 7004 7005&#x000A;</code></pre>
            
            <p>Create a <code>redis.conf</code> file inside each of the directories, from 7000 to 7005.
            As a template for your configuration file just use the small example above,
            but make sure to replace the port number <code>7000</code> with the right port number
            according to the directory name.</p>
            
            <p>Now copy your redis-server executable, <strong>compiled from the latest sources in the unstable branch at GitHub</strong>, into the <code>cluster-test</code> directory, and finally open 6 terminal tabs in your favorite terminal application.</p>
            
            <p>Start every instance like that, one every tab:</p>
            
            <pre><code>cd 7000&#x000A;../redis-server ./redis.conf&#x000A;</code></pre>
            
            <p>As you can see from the logs of every instance, since no <code>nodes.conf</code> file
            existed, every node assigns itself a new ID.</p>
            
            <pre><code>[82462] 26 Nov 11:56:55.329 * No cluster configuration found, I&#39;m 97a3a64667477371c4479320d683e4c8db5858b1&#x000A;</code></pre>
            
            <p>This ID will be used forever by this specific instance in order for the instance
            to have a unique name in the context of the cluster. Every node
            remembers every other node using this IDs, and not by IP or port.
            IP addresses and ports may change, but the unique node identifier will never
            change for all the life of the node. We call this identifier simply <strong>Node ID</strong>.</p>
            
            <span id="creating-the-cluster" class=anchor></span><h2 ><a href="#creating-the-cluster" class=anchor-link>*</a>Creating the cluster</h2>
            
            <p>Now that we have a number of instances running, we need to create our
            cluster by writing some meaningful configuration to the nodes.</p>
            
            <p>If you are using Redis 5, this is very easy to accomplish as we are helped by the Redis Cluster command line utility embedded into <code>redis-cli</code>, that can be used to create new clusters, check or reshard an existing cluster, and so forth.</p>
            
            <p>For Redis version 3 or 4, there is the older tool called <code>redis-trib.rb</code> which is very similar. You can find it in the <code>src</code> directory of the Redis source code distribution. You need to install <code>redis</code> gem to be able to run <code>redis-trib</code>.</p>
            
            <pre><code>gem install redis&#x000A;</code></pre>
            
            <p>The first example, that is, the cluster creation, will be shown using both <code>redis-cli</code> in Redis 5 and <code>redis-trib</code> in Redis 3 and 4. However all the next examples will only use <code>redis-cli</code>, since as you can see the syntax is very similar, and you can trivially change one command line into the other by using <code>redis-trib.rb help</code> to get info about the old syntax. <strong>Important:</strong> note that you can use Redis 5 <code>redis-cli</code> against Redis 4 clusters without issues if you wish.</p>
            
            <p>To create your cluster for Redis 5 with <code>redis-cli</code> simply type:</p>
            
            <pre><code>redis-cli --cluster create 127.0.0.1:7000 127.0.0.1:7001 \&#x000A;127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 \&#x000A;--cluster-replicas 1&#x000A;</code></pre>
            
            <p>Using <code>redis-trib.rb</code> for Redis 4 or 3 type:</p>
            
            <pre><code>./redis-trib.rb create --replicas 1 127.0.0.1:7000 127.0.0.1:7001 \&#x000A;127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005&#x000A;</code></pre>
            
            <p>The command used here is <strong>create</strong>, since we want to create a new cluster.
            The option <code>--cluster-replicas 1</code> means that we want a slave for every master created.
            The other arguments are the list of addresses of the instances I want to use
            to create the new cluster.</p>
            
            <p>Obviously the only setup with our requirements is to create a cluster with
            3 masters and 3 slaves.</p>
            
            <p>Redis-cli will propose you a configuration. Accept the proposed configuration by typing <strong>yes</strong>.
            The cluster will be configured and <em>joined</em>, which means, instances will be
            bootstrapped into talking with each other. Finally, if everything went well,
            you&#39;ll see a message like that:</p>
            
            <pre><code>[OK] All 16384 slots covered&#x000A;</code></pre>
            
            <p>This means that there is at least a master instance serving each of the
            16384 slots available.</p>
            
            <span id="creating-a-redis-cluster-using-the-create-cluster-script" class=anchor></span><h2 ><a href="#creating-a-redis-cluster-using-the-create-cluster-script" class=anchor-link>*</a>Creating a Redis Cluster using the create-cluster script</h2>
            
            <p>If you don&#39;t want to create a Redis Cluster by configuring and executing
            individual instances manually as explained above, there is a much simpler
            system (but you&#39;ll not learn the same amount of operational details).</p>
            
            <p>Just check <code>utils/create-cluster</code> directory in the Redis distribution.
            There is a script called <code>create-cluster</code> inside (same name as the directory
            it is contained into), it&#39;s a simple bash script. In order to start
            a 6 nodes cluster with 3 masters and 3 slaves just type the following
            commands:</p>
            
            <ol>
            <li><code>create-cluster start</code></li>
            <li><code>create-cluster create</code></li>
            </ol>
            
            <p>Reply to <code>yes</code> in step 2 when the <code>redis-cli</code> utility wants you to accept
            the cluster layout.</p>
            
            <p>You can now interact with the cluster, the first node will start at port 30001
            by default. When you are done, stop the cluster with:</p>
            
            <ol>
            <li><code>create-cluster stop</code>.</li>
            </ol>
            
            <p>Please read the <code>README</code> inside this directory for more information on how
            to run the script.</p>
            
            <span id="playing-with-the-cluster" class=anchor></span><h2 ><a href="#playing-with-the-cluster" class=anchor-link>*</a>Playing with the cluster</h2>
            
            <p>At this stage one of the problems with Redis Cluster is the lack of
            client libraries implementations.</p>
            
            <p>I&#39;m aware of the following implementations:</p>
            
            <ul>
            <li><a href="http://github.com/antirez/redis-rb-cluster">redis-rb-cluster</a> is a Ruby implementation written by me (@antirez) as a reference for other languages. It is a simple wrapper around the original redis-rb, implementing the minimal semantics to talk with the cluster efficiently.</li>
            <li><a href="https://github.com/Grokzen/redis-py-cluster">redis-py-cluster</a> A port of redis-rb-cluster to Python. Supports majority of <em>redis-py</em> functionality. Is in active development.</li>
            <li>The popular <a href="https://github.com/nrk/predis">Predis</a> has support for Redis Cluster, the support was recently updated and is in active development.</li>
            <li>The most used Java client, <a href="https://github.com/xetorthio/jedis">Jedis</a> recently added support for Redis Cluster, see the <em>Jedis Cluster</em> section in the project README.</li>
            <li><a href="https://github.com/StackExchange/StackExchange.Redis">StackExchange.Redis</a> offers support for C# (and should work fine with most .NET languages; VB, F#, etc)</li>
            <li><a href="https://github.com/thunks/thunk-redis">thunk-redis</a> offers support for Node.js and io.js, it is a thunk/promise-based redis client with pipelining and cluster.</li>
            <li><a href="https://github.com/chasex/redis-go-cluster">redis-go-cluster</a> is an implementation of Redis Cluster for the Go language using the <a href="https://github.com/garyburd/redigo">Redigo library client</a> as the base client. Implements MGET/MSET via result aggregation.</li>
            <li>The <code>redis-cli</code> utility in the unstable branch of the Redis repository at GitHub implements a very basic cluster support when started with the <code>-c</code> switch.</li>
            </ul>
            
            <p>An easy way to test Redis Cluster is either to try any of the above clients
            or simply the <code>redis-cli</code> command line utility. The following is an example
            of interaction using the latter:</p>
            
            <pre><code>$ redis-cli -c -p 7000&#x000A;redis 127.0.0.1:7000&gt; set foo bar&#x000A;-&gt; Redirected to slot [12182] located at 127.0.0.1:7002&#x000A;OK&#x000A;redis 127.0.0.1:7002&gt; set hello world&#x000A;-&gt; Redirected to slot [866] located at 127.0.0.1:7000&#x000A;OK&#x000A;redis 127.0.0.1:7000&gt; get foo&#x000A;-&gt; Redirected to slot [12182] located at 127.0.0.1:7002&#x000A;&quot;bar&quot;&#x000A;redis 127.0.0.1:7000&gt; get hello&#x000A;-&gt; Redirected to slot [866] located at 127.0.0.1:7000&#x000A;&quot;world&quot;&#x000A;</code></pre>
            
            <p><strong>Note:</strong> if you created the cluster using the script your nodes may listen
            to different ports, starting from 30001 by default.</p>
            
            <p>The redis-cli cluster support is very basic so it always uses the fact that
            Redis Cluster nodes are able to redirect a client to the right node.
            A serious client is able to do better than that, and cache the map between
            hash slots and nodes addresses, to directly use the right connection to the
            right node. The map is refreshed only when something changed in the cluster
            configuration, for example after a failover or after the system administrator
            changed the cluster layout by adding or removing nodes.</p>
            
            <span id="writing-an-example-app-with-redis-rb-cluster" class=anchor></span><h2 ><a href="#writing-an-example-app-with-redis-rb-cluster" class=anchor-link>*</a>Writing an example app with redis-rb-cluster</h2>
            
            <p>Before going forward showing how to operate the Redis Cluster, doing things
            like a failover, or a resharding, we need to create some example application
            or at least to be able to understand the semantics of a simple Redis Cluster
            client interaction.</p>
            
            <p>In this way we can run an example and at the same time try to make nodes
            failing, or start a resharding, to see how Redis Cluster behaves under real
            world conditions. It is not very helpful to see what happens while nobody
            is writing to the cluster.</p>
            
            <p>This section explains some basic usage of
            <a href="https://github.com/antirez/redis-rb-cluster">redis-rb-cluster</a> showing two
            examples. The first is the following, and is the
            <a href="https://github.com/antirez/redis-rb-cluster/blob/master/example.rb"><code>example.rb</code></a>
            file inside the redis-rb-cluster distribution:</p>
            
            <pre><code>   1  require &#39;./cluster&#39;&#x000A;   2&#x000A;   3  if ARGV.length != 2&#x000A;   4      startup_nodes = [&#x000A;   5          {:host =&gt; &quot;127.0.0.1&quot;, :port =&gt; 7000},&#x000A;   6          {:host =&gt; &quot;127.0.0.1&quot;, :port =&gt; 7001}&#x000A;   7      ]&#x000A;   8  else&#x000A;   9      startup_nodes = [&#x000A;  10          {:host =&gt; ARGV[0], :port =&gt; ARGV[1].to_i}&#x000A;  11      ]&#x000A;  12  end&#x000A;  13&#x000A;  14  rc = RedisCluster.new(startup_nodes,32,:timeout =&gt; 0.1)&#x000A;  15&#x000A;  16  last = false&#x000A;  17&#x000A;  18  while not last&#x000A;  19      begin&#x000A;  20          last = rc.get(&quot;__last__&quot;)&#x000A;  21          last = 0 if !last&#x000A;  22      rescue =&gt; e&#x000A;  23          puts &quot;error #{e.to_s}&quot;&#x000A;  24          sleep 1&#x000A;  25      end&#x000A;  26  end&#x000A;  27&#x000A;  28  ((last.to_i+1)..1000000000).each{|x|&#x000A;  29      begin&#x000A;  30          rc.set(&quot;foo#{x}&quot;,x)&#x000A;  31          puts rc.get(&quot;foo#{x}&quot;)&#x000A;  32          rc.set(&quot;__last__&quot;,x)&#x000A;  33      rescue =&gt; e&#x000A;  34          puts &quot;error #{e.to_s}&quot;&#x000A;  35      end&#x000A;  36      sleep 0.1&#x000A;  37  }&#x000A;</code></pre>
            
            <p>The application does a very simple thing, it sets keys in the form <code>foo&lt;number&gt;</code> to <code>number</code>, one after the other. So if you run the program the result is the
            following stream of commands:</p>
            
            <ul>
            <li>SET foo0 0</li>
            <li>SET foo1 1</li>
            <li>SET foo2 2</li>
            <li>And so forth...</li>
            </ul>
            
            <p>The program looks more complex than it should usually as it is designed to
            show errors on the screen instead of exiting with an exception, so every
            operation performed with the cluster is wrapped by <code>begin</code> <code>rescue</code> blocks.</p>
            
            <p>The <strong>line 14</strong> is the first interesting line in the program. It creates the
            Redis Cluster object, using as argument a list of <em>startup nodes</em>, the maximum
            number of connections this object is allowed to take against different nodes,
            and finally the timeout after a given operation is considered to be failed.</p>
            
            <p>The startup nodes don&#39;t need to be all the nodes of the cluster. The important
            thing is that at least one node is reachable. Also note that redis-rb-cluster
            updates this list of startup nodes as soon as it is able to connect with the
            first node. You should expect such a behavior with any other serious client.</p>
            
            <p>Now that we have the Redis Cluster object instance stored in the <strong>rc</strong> variable
            we are ready to use the object like if it was a normal Redis object instance.</p>
            
            <p>This is exactly what happens in <strong>line 18 to 26</strong>: when we restart the example
            we don&#39;t want to start again with <code>foo0</code>, so we store the counter inside
            Redis itself. The code above is designed to read this counter, or if the
            counter does not exist, to assign it the value of zero.</p>
            
            <p>However note how it is a while loop, as we want to try again and again even
            if the cluster is down and is returning errors. Normal applications don&#39;t need
            to be so careful.</p>
            
            <p><strong>Lines between 28 and 37</strong> start the main loop where the keys are set or
            an error is displayed.</p>
            
            <p>Note the <code>sleep</code> call at the end of the loop. In your tests you can remove
            the sleep if you want to write to the cluster as fast as possible (relatively
            to the fact that this is a busy loop without real parallelism of course, so
            you&#39;ll get the usually 10k ops/second in the best of the conditions).</p>
            
            <p>Normally writes are slowed down in order for the example application to be
            easier to follow by humans.</p>
            
            <p>Starting the application produces the following output:</p>
            
            <pre><code>ruby ./example.rb&#x000A;1&#x000A;2&#x000A;3&#x000A;4&#x000A;5&#x000A;6&#x000A;7&#x000A;8&#x000A;9&#x000A;^C (I stopped the program here)&#x000A;</code></pre>
            
            <p>This is not a very interesting program and we&#39;ll use a better one in a moment
            but we can already see what happens during a resharding when the program
            is running.</p>
            
            <span id="resharding-the-cluster" class=anchor></span><h2 ><a href="#resharding-the-cluster" class=anchor-link>*</a>Resharding the cluster</h2>
            
            <p>Now we are ready to try a cluster resharding. To do this please
            keep the example.rb program running, so that you can see if there is some
            impact on the program running. Also you may want to comment the <code>sleep</code>
            call in order to have some more serious write load during resharding.</p>
            
            <p>Resharding basically means to move hash slots from a set of nodes to another
            set of nodes, and like cluster creation it is accomplished using the
            redis-cli utility.</p>
            
            <p>To start a resharding just type:</p>
            
            <pre><code>redis-cli --cluster reshard 127.0.0.1:7000&#x000A;</code></pre>
            
            <p>You only need to specify a single node, redis-cli will find the other nodes
            automatically.</p>
            
            <p>Currently redis-cli is only able to reshard with the administrator support,
            you can&#39;t just say move 5% of slots from this node to the other one (but
            this is pretty trivial to implement). So it starts with questions. The first
            is how much a big resharding do you want to do:</p>
            
            <pre><code>How many slots do you want to move (from 1 to 16384)?&#x000A;</code></pre>
            
            <p>We can try to reshard 1000 hash slots, that should already contain a non
            trivial amount of keys if the example is still running without the sleep
            call.</p>
            
            <p>Then redis-cli needs to know what is the target of the resharding, that is,
            the node that will receive the hash slots.
            I&#39;ll use the first master node, that is, 127.0.0.1:7000, but I need
            to specify the Node ID of the instance. This was already printed in a
            list by redis-cli, but I can always find the ID of a node with the following
            command if I need:</p>
            
            <pre><code>$ redis-cli -p 7000 cluster nodes | grep myself&#x000A;97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5460&#x000A;</code></pre>
            
            <p>Ok so my target node is 97a3a64667477371c4479320d683e4c8db5858b1.</p>
            
            <p>Now you&#39;ll get asked from what nodes you want to take those keys.
            I&#39;ll just type <code>all</code> in order to take a bit of hash slots from all the
            other master nodes.</p>
            
            <p>After the final confirmation you&#39;ll see a message for every slot that
            redis-cli is going to move from a node to another, and a dot will be printed
            for every actual key moved from one side to the other.</p>
            
            <p>While the resharding is in progress you should be able to see your
            example program running unaffected. You can stop and restart it multiple times
            during the resharding if you want.</p>
            
            <p>At the end of the resharding, you can test the health of the cluster with
            the following command:</p>
            
            <pre><code>redis-cli --cluster check 127.0.0.1:7000&#x000A;</code></pre>
            
            <p>All the slots will be covered as usual, but this time the master at
            127.0.0.1:7000 will have more hash slots, something around 6461.</p>
            
            <span id="scripting-a-resharding-operation" class=anchor></span><h2 ><a href="#scripting-a-resharding-operation" class=anchor-link>*</a>Scripting a resharding operation</h2>
            
            <p>Reshardings can be performed automatically without the need to manually
            enter the parameters in an interactive way. This is possible using a command
            line like the following:</p>
            
            <pre><code>redis-cli reshard &lt;host&gt;:&lt;port&gt; --cluster-from &lt;node-id&gt; --cluster-to &lt;node-id&gt; --cluster-slots &lt;number of slots&gt; --cluster-yes&#x000A;</code></pre>
            
            <p>This allows to build some automatism if you are likely to reshard often,
            however currently there is no way for <code>redis-cli</code> to automatically
            rebalance the cluster checking the distribution of keys across the cluster
            nodes and intelligently moving slots as needed. This feature will be added
            in the future.</p>
            
            <span id="a-more-interesting-example-application" class=anchor></span><h2 ><a href="#a-more-interesting-example-application" class=anchor-link>*</a>A more interesting example application</h2>
            
            <p>The example application we wrote early is not very good.
            It writes to the cluster in a simple way without even checking if what was
            written is the right thing.</p>
            
            <p>From our point of view the cluster receiving the writes could just always
            write the key <code>foo</code> to <code>42</code> to every operation, and we would not notice at
            all.</p>
            
            <p>So in the <code>redis-rb-cluster</code> repository, there is a more interesting application
            that is called <code>consistency-test.rb</code>. It uses a set of counters, by default 1000, and sends <a href="/commands/incr">INCR</a> commands in order to increment the counters.</p>
            
            <p>However instead of just writing, the application does two additional things:</p>
            
            <ul>
            <li>When a counter is updated using <a href="/commands/incr">INCR</a>, the application remembers the write.</li>
            <li>It also reads a random counter before every write, and check if the value is what we expected it to be, comparing it with the value it has in memory.</li>
            </ul>
            
            <p>What this means is that this application is a simple <strong>consistency checker</strong>,
            and is able to tell you if the cluster lost some write, or if it accepted
            a write that we did not receive acknowledgment for. In the first case we&#39;ll
            see a counter having a value that is smaller than the one we remember, while
            in the second case the value will be greater.</p>
            
            <p>Running the consistency-test application produces a line of output every
            second:</p>
            
            <pre><code>$ ruby consistency-test.rb&#x000A;925 R (0 err) | 925 W (0 err) |&#x000A;5030 R (0 err) | 5030 W (0 err) |&#x000A;9261 R (0 err) | 9261 W (0 err) |&#x000A;13517 R (0 err) | 13517 W (0 err) |&#x000A;17780 R (0 err) | 17780 W (0 err) |&#x000A;22025 R (0 err) | 22025 W (0 err) |&#x000A;25818 R (0 err) | 25818 W (0 err) |&#x000A;</code></pre>
            
            <p>The line shows the number of <strong>R</strong>eads and <strong>W</strong>rites performed, and the
            number of errors (query not accepted because of errors since the system was
            not available).</p>
            
            <p>If some inconsistency is found, new lines are added to the output.
            This is what happens, for example, if I reset a counter manually while
            the program is running:</p>
            
            <pre><code>$ redis-cli -h 127.0.0.1 -p 7000 set key_217 0&#x000A;OK&#x000A;&#x000A;(in the other tab I see...)&#x000A;&#x000A;94774 R (0 err) | 94774 W (0 err) |&#x000A;98821 R (0 err) | 98821 W (0 err) |&#x000A;102886 R (0 err) | 102886 W (0 err) | 114 lost |&#x000A;107046 R (0 err) | 107046 W (0 err) | 114 lost |&#x000A;</code></pre>
            
            <p>When I set the counter to 0 the real value was 114, so the program reports
            114 lost writes (<a href="/commands/incr">INCR</a> commands that are not remembered by the cluster).</p>
            
            <p>This program is much more interesting as a test case, so we&#39;ll use it
            to test the Redis Cluster failover.</p>
            
            <span id="testing-the-failover" class=anchor></span><h2 ><a href="#testing-the-failover" class=anchor-link>*</a>Testing the failover</h2>
            
            <p>Note: during this test, you should take a tab open with the consistency test
            application running.</p>
            
            <p>In order to trigger the failover, the simplest thing we can do (that is also
            the semantically simplest failure that can occur in a distributed system)
            is to crash a single process, in our case a single master.</p>
            
            <p>We can identify a cluster and crash it with the following command:</p>
            
            <pre><code>$ redis-cli -p 7000 cluster nodes | grep master&#x000A;3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385482984082 0 connected 5960-10921&#x000A;2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 master - 0 1385482983582 0 connected 11423-16383&#x000A;97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5959 10922-11422&#x000A;</code></pre>
            
            <p>Ok, so 7000, 7001, and 7002 are masters. Let&#39;s crash node 7002 with the
            <strong>DEBUG SEGFAULT</strong> command:</p>
            
            <pre><code>$ redis-cli -p 7002 debug segfault&#x000A;Error: Server closed the connection&#x000A;</code></pre>
            
            <p>Now we can look at the output of the consistency test to see what it reported.</p>
            
            <pre><code>18849 R (0 err) | 18849 W (0 err) |&#x000A;23151 R (0 err) | 23151 W (0 err) |&#x000A;27302 R (0 err) | 27302 W (0 err) |&#x000A;&#x000A;... many error warnings here ...&#x000A;&#x000A;29659 R (578 err) | 29660 W (577 err) |&#x000A;33749 R (578 err) | 33750 W (577 err) |&#x000A;37918 R (578 err) | 37919 W (577 err) |&#x000A;42077 R (578 err) | 42078 W (577 err) |&#x000A;</code></pre>
            
            <p>As you can see during the failover the system was not able to accept 578 reads and 577 writes, however no inconsistency was created in the database. This may
            sound unexpected as in the first part of this tutorial we stated that Redis
            Cluster can lose writes during the failover because it uses asynchronous
            replication. What we did not say is that this is not very likely to happen
            because Redis sends the reply to the client, and the commands to replicate
            to the slaves, about at the same time, so there is a very small window to
            lose data. However the fact that it is hard to trigger does not mean that it
            is impossible, so this does not change the consistency guarantees provided
            by Redis cluster.</p>
            
            <p>We can now check what is the cluster setup after the failover (note that
            in the meantime I restarted the crashed instance so that it rejoins the
            cluster as a slave):</p>
            
            <pre><code>$ redis-cli -p 7000 cluster nodes&#x000A;3fc783611028b1707fd65345e763befb36454d73 127.0.0.1:7004 slave 3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 0 1385503418521 0 connected&#x000A;a211e242fc6b22a9427fed61285e85892fa04e08 127.0.0.1:7003 slave 97a3a64667477371c4479320d683e4c8db5858b1 0 1385503419023 0 connected&#x000A;97a3a64667477371c4479320d683e4c8db5858b1 :0 myself,master - 0 0 0 connected 0-5959 10922-11422&#x000A;3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7005 master - 0 1385503419023 3 connected 11423-16383&#x000A;3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385503417005 0 connected 5960-10921&#x000A;2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385503418016 3 connected&#x000A;</code></pre>
            
            <p>Now the masters are running on ports 7000, 7001 and 7005. What was previously
            a master, that is the Redis instance running on port 7002, is now a slave of
            7005.</p>
            
            <p>The output of the <a href="/commands/cluster-nodes">CLUSTER NODES</a> command may look intimidating, but it is actually pretty simple, and is composed of the following tokens:</p>
            
            <ul>
            <li>Node ID</li>
            <li>ip:port</li>
            <li>flags: master, slave, myself, fail, ...</li>
            <li>if it is a slave, the Node ID of the master</li>
            <li>Time of the last pending PING still waiting for a reply.</li>
            <li>Time of the last PONG received.</li>
            <li>Configuration epoch for this node (see the Cluster specification).</li>
            <li>Status of the link to this node.</li>
            <li>Slots served...</li>
            </ul>
            
            <span id="manual-failover" class=anchor></span><h2 ><a href="#manual-failover" class=anchor-link>*</a>Manual failover</h2>
            
            <p>Sometimes it is useful to force a failover without actually causing any problem
            on a master. For example in order to upgrade the Redis process of one of the
            master nodes it is a good idea to failover it in order to turn it into a slave
            with minimal impact on availability.</p>
            
            <p>Manual failovers are supported by Redis Cluster using the <a href="/commands/cluster-failover">CLUSTER FAILOVER</a>
            command, that must be executed in one of the <strong>slaves</strong> of the master you want
            to failover.</p>
            
            <p>Manual failovers are special and are safer compared to failovers resulting from
            actual master failures, since they occur in a way that avoid data loss in the
            process, by switching clients from the original master to the new master only
            when the system is sure that the new master processed all the replication stream
            from the old one.</p>
            
            <p>This is what you see in the slave log when you perform a manual failover:</p>
            
            <pre><code># Manual failover user request accepted.&#x000A;# Received replication offset for paused master manual failover: 347540&#x000A;# All master replication stream processed, manual failover can start.&#x000A;# Start of election delayed for 0 milliseconds (rank #0, offset 347540).&#x000A;# Starting a failover election for epoch 7545.&#x000A;# Failover election won: I&#39;m the new master.&#x000A;</code></pre>
            
            <p>Basically clients connected to the master we are failing over are stopped.
            At the same time the master sends its replication offset to the slave, that
            waits to reach the offset on its side. When the replication offset is reached,
            the failover starts, and the old master is informed about the configuration
            switch. When the clients are unblocked on the old master, they are redirected
            to the new master.</p>
            
            <span id="adding-a-new-node" class=anchor></span><h2 ><a href="#adding-a-new-node" class=anchor-link>*</a>Adding a new node</h2>
            
            <p>Adding a new node is basically the process of adding an empty node and then
            moving some data into it, in case it is a new master, or telling it to
            setup as a replica of a known node, in case it is a slave.</p>
            
            <p>We&#39;ll show both, starting with the addition of a new master instance.</p>
            
            <p>In both cases the first step to perform is <strong>adding an empty node</strong>.</p>
            
            <p>This is as simple as to start a new node in port 7006 (we already used
            from 7000 to 7005 for our existing 6 nodes) with the same configuration
            used for the other nodes, except for the port number, so what you should
            do in order to conform with the setup we used for the previous nodes:</p>
            
            <ul>
            <li>Create a new tab in your terminal application.</li>
            <li>Enter the <code>cluster-test</code> directory.</li>
            <li>Create a directory named <code>7006</code>.</li>
            <li>Create a redis.conf file inside, similar to the one used for the other nodes but using 7006 as port number.</li>
            <li>Finally start the server with <code>../redis-server ./redis.conf</code></li>
            </ul>
            
            <p>At this point the server should be running.</p>
            
            <p>Now we can use <strong>redis-cli</strong> as usual in order to add the node to
            the existing cluster.</p>
            
            <pre><code>redis-cli --cluster add-node 127.0.0.1:7006 127.0.0.1:7000&#x000A;</code></pre>
            
            <p>As you can see I used the <strong>add-node</strong> command specifying the address of the
            new node as first argument, and the address of a random existing node in the
            cluster as second argument.</p>
            
            <p>In practical terms redis-cli here did very little to help us, it just
            sent a <a href="/commands/cluster-meet">CLUSTER MEET</a> message to the node, something that is also possible
            to accomplish manually. However redis-cli also checks the state of the
            cluster before to operate, so it is a good idea to perform cluster operations
            always via redis-cli even when you know how the internals work.</p>
            
            <p>Now we can connect to the new node to see if it really joined the cluster:</p>
            
            <pre><code>redis 127.0.0.1:7006&gt; cluster nodes&#x000A;3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 127.0.0.1:7001 master - 0 1385543178575 0 connected 5960-10921&#x000A;3fc783611028b1707fd65345e763befb36454d73 127.0.0.1:7004 slave 3e3a6cb0d9a9a87168e266b0a0b24026c0aae3f0 0 1385543179583 0 connected&#x000A;f093c80dde814da99c5cf72a7dd01590792b783b :0 myself,master - 0 0 0 connected&#x000A;2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543178072 3 connected&#x000A;a211e242fc6b22a9427fed61285e85892fa04e08 127.0.0.1:7003 slave 97a3a64667477371c4479320d683e4c8db5858b1 0 1385543178575 0 connected&#x000A;97a3a64667477371c4479320d683e4c8db5858b1 127.0.0.1:7000 master - 0 1385543179080 0 connected 0-5959 10922-11422&#x000A;3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 127.0.0.1:7005 master - 0 1385543177568 3 connected 11423-16383&#x000A;</code></pre>
            
            <p>Note that since this node is already connected to the cluster it is already
            able to redirect client queries correctly and is generally speaking part of
            the cluster. However it has two peculiarities compared to the other masters:</p>
            
            <ul>
            <li>It holds no data as it has no assigned hash slots.</li>
            <li>Because it is a master without assigned slots, it does not participate in the election process when a slave wants to become a master.</li>
            </ul>
            
            <p>Now it is possible to assign hash slots to this node using the resharding
            feature of <code>redis-cli</code>. It is basically useless to show this as we already
            did in a previous section, there is no difference, it is just a resharding
            having as a target the empty node.</p>
            
            <span id="adding-a-new-node-as-a-replica" class=anchor></span><h2 ><a href="#adding-a-new-node-as-a-replica" class=anchor-link>*</a>Adding a new node as a replica</h2>
            
            <p>Adding a new Replica can be performed in two ways. The obvious one is to
            use redis-cli again, but with the --cluster-slave option, like this:</p>
            
            <pre><code>redis-cli --cluster add-node 127.0.0.1:7006 127.0.0.1:7000 --cluster-slave&#x000A;</code></pre>
            
            <p>Note that the command line here is exactly like the one we used to add
            a new master, so we are not specifying to which master we want to add
            the replica. In this case what happens is that redis-cli will add the new
            node as replica of a random master among the masters with less replicas.</p>
            
            <p>However you can specify exactly what master you want to target with your
            new replica with the following command line:</p>
            
            <pre><code>redis-cli --cluster add-node 127.0.0.1:7006 127.0.0.1:7000 --cluster-slave --cluster-master-id 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e&#x000A;</code></pre>
            
            <p>This way we assign the new replica to a specific master.</p>
            
            <p>A more manual way to add a replica to a specific master is to add the new
            node as an empty master, and then turn it into a replica using the
            <a href="/commands/cluster-replicate">CLUSTER REPLICATE</a> command. This also works if the node was added as a slave
            but you want to move it as a replica of a different master.</p>
            
            <p>For example in order to add a replica for the node 127.0.0.1:7005 that is
            currently serving hash slots in the range 11423-16383, that has a Node ID
            3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e, all I need to do is to connect
            with the new node (already added as empty master) and send the command:</p>
            
            <pre><code>redis 127.0.0.1:7006&gt; cluster replicate 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e&#x000A;</code></pre>
            
            <p>That&#39;s it. Now we have a new replica for this set of hash slots, and all
            the other nodes in the cluster already know (after a few seconds needed to
            update their config). We can verify with the following command:</p>
            
            <pre><code>$ redis-cli -p 7000 cluster nodes | grep slave | grep 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e&#x000A;f093c80dde814da99c5cf72a7dd01590792b783b 127.0.0.1:7006 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543617702 3 connected&#x000A;2938205e12de373867bf38f1ca29d31d0ddb3e46 127.0.0.1:7002 slave 3c3a0c74aae0b56170ccb03a76b60cfe7dc1912e 0 1385543617198 3 connected&#x000A;</code></pre>
            
            <p>The node 3c3a0c... now has two slaves, running on ports 7002 (the existing one) and 7006 (the new one).</p>
            
            <span id="removing-a-node" class=anchor></span><h2 ><a href="#removing-a-node" class=anchor-link>*</a>Removing a node</h2>
            
            <p>To remove a slave node just use the <code>del-node</code> command of redis-cli:</p>
            
            <pre><code>redis-cli --cluster del-node 127.0.0.1:7000 `&lt;node-id&gt;`&#x000A;</code></pre>
            
            <p>The first argument is just a random node in the cluster, the second argument
            is the ID of the node you want to remove.</p>
            
            <p>You can remove a master node in the same way as well, <strong>however in order to
            remove a master node it must be empty</strong>. If the master is not empty you need
            to reshard data away from it to all the other master nodes before.</p>
            
            <p>An alternative to remove a master node is to perform a manual failover of it
            over one of its slaves and remove the node after it turned into a slave of the
            new master. Obviously this does not help when you want to reduce the actual
            number of masters in your cluster, in that case, a resharding is needed.</p>
            
            <span id="replicas-migration" class=anchor></span><h2 ><a href="#replicas-migration" class=anchor-link>*</a>Replicas migration</h2>
            
            <p>In Redis Cluster it is possible to reconfigure a slave to replicate with a
            different master at any time just using the following command:</p>
            
            <pre><code>CLUSTER REPLICATE &lt;master-node-id&gt;&#x000A;</code></pre>
            
            <p>However there is a special scenario where you want replicas to move from one
            master to another one automatically, without the help of the system administrator.
            The automatic reconfiguration of replicas is called <em>replicas migration</em> and is
            able to improve the reliability of a Redis Cluster.</p>
            
            <p>Note: you can read the details of replicas migration in the <a href="/topics/cluster-spec">Redis Cluster Specification</a>, here we&#39;ll only provide some information about the
            general idea and what you should do in order to benefit from it.</p>
            
            <p>The reason why you may want to let your cluster replicas to move from one master
            to another under certain condition, is that usually the Redis Cluster is as
            resistant to failures as the number of replicas attached to a given master.</p>
            
            <p>For example a cluster where every master has a single replica can&#39;t continue
            operations if the master and its replica fail at the same time, simply because
            there is no other instance to have a copy of the hash slots the master was
            serving. However while netsplits are likely to isolate a number of nodes
            at the same time, many other kind of failures, like hardware or software failures
            local to a single node, are a very notable class of failures that are unlikely
            to happen at the same time, so it is possible that in your cluster where
            every master has a slave, the slave is killed at 4am, and the master is killed
            at 6am. This still will result in a cluster that can no longer operate.</p>
            
            <p>To improve reliability of the system we have the option to add additional
            replicas to every master, but this is expensive. Replica migration allows to
            add more slaves to just a few masters. So you have 10 masters with 1 slave
            each, for a total of 20 instances. However you add, for example, 3 instances
            more as slaves of some of your masters, so certain masters will have more
            than a single slave.</p>
            
            <p>With replicas migration what happens is that if a master is left without
            slaves, a replica from a master that has multiple slaves will migrate to
            the <em>orphaned</em> master. So after your slave goes down at 4am as in the example
            we made above, another slave will take its place, and when the master
            will fail as well at 5am, there is still a slave that can be elected so that
            the cluster can continue to operate.</p>
            
            <p>So what you should know about replicas migration in short?</p>
            
            <ul>
            <li>The cluster will try to migrate a replica from the master that has the greatest number of replicas in a given moment.</li>
            <li>To benefit from replica migration you have just to add a few more replicas to a single master in your cluster, it does not matter what master.</li>
            <li>There is a configuration parameter that controls the replica migration feature that is called <code>cluster-migration-barrier</code>: you can read more about it in the example <code>redis.conf</code> file provided with Redis Cluster.</li>
            </ul>
            
            <span id="upgrading-nodes-in-a-redis-cluster" class=anchor></span><h2 ><a href="#upgrading-nodes-in-a-redis-cluster" class=anchor-link>*</a>Upgrading nodes in a Redis Cluster</h2>
            
            <p>Upgrading slave nodes is easy since you just need to stop the node and restart
            it with an updated version of Redis. If there are clients scaling reads using
            slave nodes, they should be able to reconnect to a different slave if a given
            one is not available.</p>
            
            <p>Upgrading masters is a bit more complex, and the suggested procedure is:</p>
            
            <ol>
            <li>Use CLUSTER FAILOVER to trigger a manual failover of the master to one of its slaves (see the &quot;Manual failover&quot; section of this documentation).</li>
            <li>Wait for the master to turn into a slave.</li>
            <li>Finally upgrade the node as you do for slaves.</li>
            <li>If you want the master to be the node you just upgraded, trigger a new manual failover in order to turn back the upgraded node into a master.</li>
            </ol>
            
            <p>Following this procedure you should upgrade one node after the other until
            all the nodes are upgraded.</p>
            
            <span id="migrating-to-redis-cluster" class=anchor></span><h2 ><a href="#migrating-to-redis-cluster" class=anchor-link>*</a>Migrating to Redis Cluster</h2>
            
            <p>Users willing to migrate to Redis Cluster may have just a single master, or
            may already using a preexisting sharding setup, where keys
            are split among N nodes, using some in-house algorithm or a sharding algorithm
            implemented by their client library or Redis proxy.</p>
            
            <p>In both cases it is possible to migrate to Redis Cluster easily, however
            what is the most important detail is if multiple-keys operations are used
            by the application, and how. There are three different cases:</p>
            
            <ol>
            <li>Multiple keys operations, or transactions, or Lua scripts involving multiple keys, are not used. Keys are accessed independently (even if accessed via transactions or Lua scripts grouping multiple commands, about the same key, together).</li>
            <li>Multiple keys operations, transactions, or Lua scripts involving multiple keys are used but only with keys having the same <strong>hash tag</strong>, which means that the keys used together all have a <code>{...}</code> sub-string that happens to be identical. For example the following multiple keys operation is defined in the context of the same hash tag: <code>SUNION {user:1000}.foo {user:1000}.bar</code>.</li>
            <li>Multiple keys operations, transactions, or Lua scripts involving multiple keys are used with key names not having an explicit, or the same, hash tag.</li>
            </ol>
            
            <p>The third case is not handled by Redis Cluster: the application requires to
            be modified in order to don&#39;t use multi keys operations or only use them in
            the context of the same hash tag.</p>
            
            <p>Case 1 and 2 are covered, so we&#39;ll focus on those two cases, that are handled
            in the same way, so no distinction will be made in the documentation.</p>
            
            <p>Assuming you have your preexisting data set split into N masters, where
            N=1 if you have no preexisting sharding, the following steps are needed
            in order to migrate your data set to Redis Cluster:</p>
            
            <ol>
            <li>Stop your clients. No automatic live-migration to Redis Cluster is currently possible. You may be able to do it orchestrating a live migration in the context of your application / environment.</li>
            <li>Generate an append only file for all of your N masters using the BGREWRITEAOF command, and waiting for the AOF file to be completely generated.</li>
            <li>Save your AOF files from aof-1 to aof-N somewhere. At this point you can stop your old instances if you wish (this is useful since in non-virtualized deployments you often need to reuse the same computers).</li>
            <li>Create a Redis Cluster composed of N masters and zero slaves. You&#39;ll add slaves later. Make sure all your nodes are using the append only file for persistence.</li>
            <li>Stop all the cluster nodes, substitute their append only file with your pre-existing append only files, aof-1 for the first node, aof-2 for the second node, up to aof-N.</li>
            <li>Restart your Redis Cluster nodes with the new AOF files. They&#39;ll complain that there are keys that should not be there according to their configuration.</li>
            <li>Use <code>redis-cli --cluster fix</code> command in order to fix the cluster so that keys will be migrated according to the hash slots each node is authoritative or not.</li>
            <li>Use <code>redis-cli --cluster check</code> at the end to make sure your cluster is ok.</li>
            <li>Restart your clients modified to use a Redis Cluster aware client library.</li>
            </ol>
            
            <p>There is an alternative way to import data from external instances to a Redis
            Cluster, which is to use the <code>redis-cli --cluster import</code> command.</p>
            
            <p>The command moves all the keys of a running instance (deleting the keys from
            the source instance) to the specified pre-existing Redis Cluster. However
            note that if you use a Redis 2.8 instance as source instance the operation
            may be slow since 2.8 does not implement migrate connection caching, so you
            may want to restart your source instance with a Redis 3.x version before
            to perform such operation.</p>
          </article>
        </div>
      </div>
      <footer class='site-footer'>
        <div class='container'>
          <p>
            This website is
            <a href="https://github.com/antirez/redis-io">open source software</a>.
            See all <a href="/topics/sponsors">credits</a>.
          </p>
          <div class='sponsor'>
            Sponsored by
            <a href='https://redislabs.com/'>
              <img alt='Redis Labs' height='25' src='/images/redislabs.png' title='Get a Managed Redis' width='128'>
            </a>
          </div>
        </div>
      </footer>
    </div>
    <script src='https://ajax.googleapis.com/ajax/libs/jquery/1.4/jquery.min.js'></script>
    <script src='/app.js?1480208557'></script>
  </body>
</html>
